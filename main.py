#-*- coding: UTF-8 -*-
__author__ = "https://github.com/lopinx"
# å¯¼å‡ºåŒ…ï¼š uv pip freeze | uv pip compile - -o requirements.txt
# =========================================================================================================================
# pip install httpx[http2,http3] keybert scikit-learn jieba nltk rank-bm25 fuzzywuzzy python-Levenshtein markdown pygments pymdown-extensions markdownify openai pandas python-slugify pypinyin tomlkit
# æœ±é›€å¤§æ¨¡å‹æ£€æµ‹ï¼šhttps://matrix.tencent.com/ai-detect/
# æœ±é›€å¤§æ¨¡å‹ç»­æ¯ï¼š`localStorage.setItem('fp',Array.from({ length: 32 }, () => '0123456789abcdef'[Math.floor(Math.random() * 16)]).join(''))`
#
# ==========================================================================================================================
import json
import logging
import random
import re
import time
import uuid
from collections import Counter, OrderedDict, defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import httpx
import jieba
import jieba.analyse
import markdown  # markdownè½¬html
import nltk
import numpy as np
import tomlkit
from fuzzywuzzy import process
from keybert import KeyBERT
from markdownify import markdownify  # htmlè½¬markdown
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
# from nltk.util import everygrams
from openai import OpenAI
from pypinyin import Style, lazy_pinyin
from rank_bm25 import BM25Okapi
from sklearn.feature_extraction.text import TfidfVectorizer
from slugify import slugify

#  ##########################################################################################################################
# å½“å‰å·¥ä½œç›®å½•,é…ç½®æ–‡ä»¶
WorkDIR = Path(__file__).resolve().parent
config = json.load(open(WorkDIR/"config.json", 'r', encoding='utf-8'))
# ä¸‹è½½åˆ†è¯åº“æ•°æ®ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ï¼‰
try:
    nltk.corpus.stopwords.words()
except LookupError:
    nltk.download('stopwords')
try:
    nltk.sent_tokenize("Test sentence")
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')
# markdownåº“æ‰©å±•
markdown_extensions = [
    'fenced_code',                                          # ä»£ç å—ï¼ˆä¸‰ä¸ªåå¼•å·åŒ…è£¹ï¼‰
    'codehilite',                                           # ä»£ç é«˜äº®ï¼ˆéœ€å®‰è£… pygmentsï¼‰
    'tables',                                               # è¡¨æ ¼æ”¯æŒ
    'footnotes',                                            # è„šæ³¨
    'sane_lists',                                           # æ™ºèƒ½åˆ—è¡¨æ ¼å¼åŒ–
    'nl2br',                                                # æ¢è¡Œè½¬<br>æ ‡ç­¾
    # æ”¯æŒç¼©å†™ï¼ˆå¦‚ [[NASA|National Aeronautics...]],<a title="National Aeronautics and Space Administration" href="http://www.nasa.gov/">NASA</a> ï¼‰
    'abbr',                                                 # æ”¯æŒç¼©å†™
    'toc',                                                  # ç›®å½•
    # 'strikethrough',                                        # åˆ é™¤çº¿
    'pymdownx.tasklist'                                     # ä»»åŠ¡åˆ—è¡¨(å¾…åŠåˆ—è¡¨ï¼ˆéœ€ pymdown-extensionsï¼‰)    
]
markdown_extconfigs = {
    'codehilite': {'linenums': True, 'pygments_style': 'monokai'}
}

# è¿œç¨‹ https://res.cdn.issem.cn/ChineseStopWords.txt, å¹¶å°†å†…å®¹è½¬æ¢ä¸ºåˆ—è¡¨
try:
    cn_stopk, en_stopk = [[*map(str.strip,filter(str.strip,(WorkDIR/config['stopk'][l]).open(encoding='utf-8')))] for l in ('cn','en')]
except:
    cn_stopk, en_stopk = [], []
# æ—¥å¿—è®°å½•é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - L%(lineno)d - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logging.info(f"ğŸš€ å¯åŠ¨ç¨‹åº {__author__}")
# ##########################################################################################################################

class WeChatMP():
    def __init__(self, wechat: Dict, gpts: List[Dict]) -> None:
        self.gpts = gpts
        self.wechat = wechat

    """ä»¤ç‰Œï¼šè¿”å›çŠ¶æ€å’Œä»¤ç‰Œ"""
    def get_wechatmp_token(self) -> Tuple[bool, str]:
        try:
            with (Path(WorkDIR) / f"{self.wechat.get('appid')}.json").open("r", encoding="utf-8") as f:
                data = json.load(f)
            expires_in = data.get("expires_in", 0)
            # (è®¾ç½®æå‰200ç§’è¿‡æœŸï¼Œç”¨æ¥æŠµæ‰£ç¨‹åºè¿è¡Œä¸­è€—æ—¶)
            if expires_in > 0 and expires_in > int(time.time()) + 200:        
                return True, data.get("access_token")
        except (FileNotFoundError, json.JSONDecodeError):
            try:
                with httpx.Client() as client:
                    url = f"{self.wechat['baseurl']}/cgi-bin/token"
                    params = {
                        'grant_type': 'client_credential',
                        'appid': self.wechat.get('appid'),
                        'secret': self.wechat.get('secret')
                    } 
                    res = client.get(url, params=params)
                    res.raise_for_status()
                    data = res.json()
                    if "access_token" in data:
                        # ä¿®æ”¹è¿‡æœŸæ—¶é—´ï¼šå°†è¿”å›çš„è¿‡æœŸæ—¶é•¿ä¿®æ”¹ä¸ºå½“å‰æ—¶é—´æˆ³åŠ ä¸Šè¿”å›çš„è¿‡æœŸæ—¶é•¿
                        data["expires_in"] = int(time.time()) + data["expires_in"]
                        with (Path(WorkDIR) / f"{self.wechat.get('appid')}.json").open("w", encoding="utf-8") as f:
                            json.dump(data, f)
                        return True, data.get("access_token")
                    else:
                        return False, data.get("errmsg", "")
            except (httpx.RequestError, httpx.HTTPStatusError) as e:
                return False, "è·å–access_tokenå¤±è´¥"
            

    """è‰ç¨¿:è¿”å›çŠ¶æ€å’Œæ–‡ç« ID"""
    def get_wechatmp_draft(self, articles: List[Dict]) -> Tuple[bool, str]:
        token = self.get_wechatmp_token()[1]
        if not token[0]:
            return False, "è·å–access_tokenå¤±è´¥"
        _drafts = []
        # ä¸Šä¼ å›¾ç‰‡ è¿”å›(sucess, media_id)
        @staticmethod
        def upload_wechatmp_image(token: str, file_path: str) -> Tuple[bool, str]:
            try:
                with open(file=file_path, mode='rb') as fp:
                    url = f"{self.wechat['baseurl']}/cgi-bin/material/add_material"
                    params = {
                        'access_token': token,
                        'type': 'image'
                    }         
                    files = {'media': fp}
                    with httpx.Client() as client:
                        res = client.post(url=url, params=params, files=files)
                        res.raise_for_status()
                        try:
                            data = res.json()
                            if "media_id" in data:
                                 return True, data.get("media_id", "")
                            else:
                                 return False, data.get("errmsg", "")
                        except json.JSONDecodeError:
                            return False, "å“åº”å†…å®¹æ— æ³•è§£æä¸ºJSON"
            except (httpx.RequestError, httpx.HTTPStatusError, FileNotFoundError, Exception) as e:
                # logging.error()
                return False, f"å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {e}"

        for article in articles:
            try:
                # åŒ¹é…æœ€ç›¸ä¼¼çš„é”®
                media_ids = self.wechat.get("media_ids")
                _titmatch = process.extractOne(article["title"], media_ids.keys())
                image_stauts, cover_media_id = True, (media_ids[_titmatch[0]] if _titmatch else random.choice(media_ids.values()))
            except:
                image_stauts, cover_media_id = upload_wechatmp_image(token, article["cover_path"]) 

            if not image_stauts: return False

            _drafts.append({
                "article_type": "news",
                "title": article["title"],
                "thumb_media_id": cover_media_id,                   # å›¾æ–‡æ¶ˆæ¯çš„å°é¢å›¾ç‰‡ç´ æidï¼ˆå¿…é¡»æ˜¯æ°¸ä¹…MediaIDï¼‰
                "content": article["content"],                      #{company_info}{top_ad}{bottom_ad}
                "content_source_url": self.wechat.get("source_url"),    # å›¾æ–‡æ¶ˆæ¯çš„åŸæ–‡åœ°å€ï¼Œå³ç‚¹å‡»â€œé˜…è¯»åŸæ–‡â€åçš„URL
                "need_open_comment": 1,                             # æ˜¯å¦æ‰“å¼€è¯„è®ºï¼Œ0ä¸æ‰“å¼€ï¼Œ1æ‰“å¼€
                "only_fans_can_comment": 0,                         # æ˜¯å¦ç²‰ä¸æ‰å¯è¯„è®ºï¼Œ0æ‰€æœ‰äººå¯è¯„è®ºï¼Œ1ç²‰ä¸æ‰å¯è¯„è®º
                "author": self.wechat.get("author"),                    # éå¿…å¡«
                # "digest": "digest",                               # éå¿…å¡«(ä»…æœ‰å•å›¾æ–‡æ¶ˆæ¯æ‰æœ‰æ‘˜è¦ï¼Œåˆ™é»˜è®¤æŠ“å–æ­£æ–‡å‰54ä¸ªå­—ã€‚)
                # "pic_crop_235_1": "X1_Y1_X2_Y2",                  # å°é¢å›¾ç‰‡çš„è£å‰ªåæ ‡ï¼Œåæ ‡ä»¥å›¾ç‰‡å·¦ä¸Šè§’ä¸ºåŸç‚¹ï¼Œxè½´å‘å³ï¼Œyè½´å‘ä¸‹
                # "pic_crop_1_1": "X1_Y1_X2_Y2"                     # å°é¢å›¾ç‰‡çš„è£å‰ªåæ ‡ï¼Œåæ ‡ä»¥å›¾ç‰‡å·¦ä¸Šè§’ä¸ºåŸç‚¹ï¼Œxè½´å‘å³ï¼Œyè½´å‘ä¸‹
            })

        drafts = {"articles": _drafts}
        # ç›´æ¥post json.dumps(drafts) ä¼šæœ‰ä¹±ç     
        post_data = json.dumps(drafts, ensure_ascii=False).encode('utf-8')
        try:
            with httpx.Client() as client:
                url = f"{self.wechat['baseurl']}/cgi-bin/draft/add"
                params = {
                    'access_token': token
                }     
                res = client.post(url=url, params=params, content=post_data)
                res.raise_for_status()
                try:
                    data = res.json()
                    if "media_id" in data:
                        return True, data.get("media_id", "")
                    else:
                        return False, data.get("errmsg", "")
                except json.JSONDecodeError:
                    return False, "å“åº”å†…å®¹æ— æ³•è§£æä¸ºJSON"
        except Exception as e:
            return False, f"è‰ç¨¿ä¸Šä¼ å¤±è´¥: {e}"
        

    """å‘å¸ƒ:è¿”å›çŠ¶æ€å’Œæ–‡ç« ID"""
    def get_wechatmp_publish(self, draft_id: int) -> Tuple[bool, str]:
        token = self.get_wechatmp_token()[1]
        if not token[0]:
            return False, "è·å–access_tokenå¤±è´¥"
        try:
            with httpx.Client() as client:
                url = f"{self.wechat['baseurl']}/cgi-bin/freepublish/add"
                params = {
                    'access_token': token
                }
                res = client.post(url=url, params=params ,json={"media_id": draft_id})
                res.raise_for_status()
                try:
                    data = res.json()
                    if "media_id" in data and data.get("errcode") == 0:
                        return True, data.get("publish_id", "")
                    else:
                        return False, data.get("errmsg", "")
                except json.JSONDecodeError:
                    return False, "å“åº”å†…å®¹æ— æ³•è§£æä¸ºJSON"
        except Exception as e:
            return False, f"æ–‡ç« å‘å¸ƒå¤±è´¥ï¼š{e}" 


# ========================================================================================================================
# ä»¥ä¸‹ä¸ºæ‹“å±•ç±»ï¼ŒæŒ‰éœ€ä½¿ç”¨
# ========================================================================================================================

    """å‘å¸ƒåˆ°æ–‡ç« åˆ°å…¬ä¼—å·ä¸Š"""
    def main(self):
        # 1. è·å–æ–‡ç« æ•°æ®
        articles = Extensions.get_article_data(self.wechat)
        if not articles:
            return False
        if self.wechat['draft']:# 2 ä¿å­˜å¾®ä¿¡è‰ç¨¿
            draft_status, draft_id = self.get_wechatmp_draft(articles)
            if not draft_status: 
                return False
        if self.wechat['publish']:# 3 å¾®ä¿¡æ¨é€å‘å¸ƒ
            publish_status, publish_id = self.get_wechatmp_publish(draft_id)
            if not publish_status: 
                return False
            elif len(publish_id) > 16:
                return "å‘å¸ƒæˆåŠŸ", publish_id
            else: 
                return "å‘å¸ƒå¤±è´¥", publish_id


class Extensions:
    """è·å–æ‰€æœ‰å¯ç”¨GPTé…ç½®"""
    @staticmethod
    def get_gpt_config(gpts: List[Dict]) -> List[Dict]:
        new_gpts = []
        for entry in gpts:
            if any('sk-or-' in key for key in entry['apikey']) and len(entry["models"])==0:
                get_models = Extensions.get_openrouter_models(
                    entry["baseurl"],
                    entry["fee_type"], 
                    entry["from_type"], 
                    entry["to_type"], 
                    entry["min_tokens"]
                )
                _entry = {
                    "baseurl": entry["baseurl"],
                    "apikey": entry["apikey"],
                    "models": get_models
                }
                new_gpts.append(_entry)
            else:
                new_gpts.append(entry)
        return new_gpts
    

    """è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    @staticmethod
    def get_openrouter_models(
        baseurl: str = "https://openrouter.ai/api/v1", 
        fee_type: str = "free", 
        from_type: str = "text", 
        to_type: str = "text", 
        min_tokens: int = 4096
    ) -> List[Dict]:
        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        type_models = []
        try:
            with httpx.Client() as client:
                resp = client.get(url=f"{baseurl}/models")
                resp.raise_for_status()
                models_data = resp.json()
        except Exception as e:
            return []

        _free, _paid = [], []
        # ç¬¬ä¸€æ­¥ï¼šç­›é€‰æ¨¡å‹ç±»å‹
        for _f in models_data.get("data", []):
            pricing = _f.get("pricing")
            if pricing.get("prompt") == "0" and pricing.get("completion") == "0":
                _free.append(_f)
            else:
                _paid.append(_f)   
        models_detail = {"paid": _paid, "free": _free}.get(fee_type, _free + _paid)
            
        # ç¬¬äºŒæ­¥ï¼šç­›é€‰æ¨¡å‹ç±»å‹
        for model in models_detail:
            modality = model.get("architecture").get("modality")
            max_completion_tokens = model.get("top_provider").get("max_completion_tokens")
            _tokens = max_completion_tokens is None or max_completion_tokens > min_tokens
            _type = from_type + "->" + to_type
            if modality == _type and _tokens:
                # type_models.append(model)
                type_models.append({
                    "baseurl": "https://openrouter.ai/api/v1",
                    "id": model.get("id"),
                    "name": model.get("name"),
                    "pricing": model.get("pricing"),
                    "max_tokens": max_completion_tokens,
                    "context_length": model.get("context_length")
                })
            else:
                continue
        
        # return list(type_models)
        return [models.get("id") for models in type_models]
    
    
    """ç”¨åˆ†è¯å™¨æå–å…ƒå…³é”®è¯"""
    @staticmethod
    def extract_article_keywords(content: str, require_words: List[str] ) -> List[str]:
        if not content: return []
        # å¤„ç†æˆMarkdownæ ¼å¼
        try:
            html = markdown.markdown(content)
            if not (('<' in html and '>' in html) and html.strip() != content.strip()):
                content = markdownify(content)
        except Exception:
            content = markdownify(content)
        # æ¸…ç†éæ³•å­—ç¬¦ï¼ˆä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€å¸¸è§ç¬¦å·ï¼‰
        # éœ€è¦åŒ¹é…è‹±æ–‡å•è¯ã€ä¸­æ–‡ã€ä¸“æœ‰è‹±æ–‡ç¼©å†™ï¼ˆå¦‚ NASAã€U.S.A.ï¼‰ä»¥åŠç‰¹å®šç¬¦å·
        content = re.sub(
            r'[^a-zA-Z0-9\u4e00-\u9fa5\s\-.\'@#$%&*+/:;=?~(){}$`_ã€ã€‚ï¼Œã€Šã€‹ï¼Ÿï¼â€œâ€â€˜â€™ï¼ˆï¼‰â€”â€¦]',
            ' ',                                                # ç”¨ç©ºæ ¼æ›¿ä»£éæ³•å­—ç¬¦
            content,                                            # æ›¿æ¢ç›®æ ‡å­—ç¬¦ä¸²
            flags=re.UNICODE
        ).strip()
        # åˆ¤æ–­æ–‡æœ¬è¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰
        cn_lang = any(
            (u'\u4e00' <= char <= u'\u9fa5') or
            (u'\u3400' <= char <= u'\u4DBF') or
            (u'\U00020000' <= char <= u'\U0002A6DF')
            for char in content
        )
        # å¯é€‰åœç”¨è¯
        if not cn_lang:
            stop_words = set(stopwords.words('english')).union(en_stopk)
        else:
            stop_words = set(stopwords.words('chinese')).union(cn_stopk)
        # ===============================================================================================================
        # Keybert ç®—æ³•ï¼ˆè‹±æ–‡ï¼‰ / TextRank + jieba ç®—æ³•ï¼ˆä¸­æ–‡ï¼‰
        # ===============================================================================================================
        if not cn_lang:
            # è‹±æ–‡å¤„ç†ï¼šKeyBERT + è¯­ä¹‰ä¼˜å…ˆ
            kw_model = KeyBERT()
            keywords = kw_model.extract_article_keywords(
                content.lower(),
                keyphrase_ngram_range=(1, 6),
                stop_words=stop_words,
                use_mmr=True,
                diversity=0.5
            )
            vectorizer = TfidfVectorizer(ngram_range=(1, 6))
            tfidf = vectorizer.fit_transform([content])
            vocab = vectorizer.vocabulary_  # è·å–è¯æ±‡è¡¨
            # ç»¼åˆè¯„åˆ†ï¼šBERTç½®ä¿¡åº¦ Ã— TF-IDF
            scores = {
                word: score * tfidf[0, vocab.get(word, -1)]
                for word, score in keywords
                if word in vocab and word.lower() in content.lower()  # ç¡®ä¿å…³é”®è¯åœ¨æ–‡ç« ä¸­
            }
        else:
            # ä¸­æ–‡å¤„ç†ï¼šTextRank + è¯å¯†åº¦
            # å°†ç”¨æˆ·è¯é€ä¸ªæ·»åŠ åˆ°jiebaè¯å…¸ï¼ˆç¡®ä¿çŸ­è¯­ä¸è¢«æ‹†åˆ†ï¼‰
            list(map(jieba.add_word, require_words))
            keywords = [word for word in jieba.cut(content) if word not in stop_words or word in require_words]
            vectorizer = TfidfVectorizer(ngram_range=(1, 6), token_pattern=r'[^\s]+')
            tfidf_matrix = vectorizer.fit_transform([' '.join(keywords)])
            vocab = vectorizer.vocabulary_  
            tfidf = tfidf_matrix.toarray()[0]
            # TextRankæƒé‡ï¼ˆå…è®¸çŸ­è¯­ï¼‰
            text_rank = {k: v for k, v in jieba.analyse.extract_tags(' '.join(keywords), topK=500, withWeight=True, allowPOS=())}
            scores = {
                word: text_rank.get(word, 0) * tfidf[vocab[word]] 
                for word in keywords 
                 if word in vocab and word in content  # ç¡®ä¿å…³é”®è¯åœ¨æ–‡ç« ä¸­
            }
        # å¼ºåˆ¶åˆå¹¶ç”¨æˆ·è¯å…¸ä¸­çš„è¯ï¼ˆä¸­è‹±æ–‡ç»Ÿä¸€å¤„ç†ï¼‰
        for key in require_words:
            word = key.lower()
            if word in content.lower() and word not in scores:  # ç¡®ä¿ç”¨æˆ·è¯åœ¨æ–‡ç« ä¸­
                scores[word] = tfidf[vocab[word]] if cn_lang else 1.0 * tfidf[0, vocab[word]]
        # å¼ºåˆ¶åŒ…å«ç”¨æˆ·æä¾›è¯ï¼ˆç»Ÿä¸€é€»è¾‘ï¼‰
        for req_word in (r.lower() for r in require_words):
            if req_word in content.lower():  # ç¡®ä¿ç”¨æˆ·è¯åœ¨æ–‡ç« ä¸­
                current = scores.get(req_word, 0)
                scores[req_word] = current * 2 if current else max(scores.values(), default=0) * 2 + 1

        # ç”Ÿæˆæ’åºåçš„å…³é”®è¯åˆ—è¡¨
        t_k = sorted(scores.keys(), key=lambda k: (-scores[k], -len(k)))
        # è¿‡æ»¤ä¸ç¬¦åˆæ¡ä»¶çš„å…³é”®è¯
        filter_pattern = re.compile(r'^[\W_]+$|^\d+(?:\.\d+)?%?$|^\d+[eE][+-]?\d+$')
        return [k for k in t_k if len(k) >= 2 and not filter_pattern.fullmatch(k)]
        # ===============================================================================================================


    """é€šè¿‡AIç”Ÿæˆæ ‡é¢˜æˆ–å†…å®¹"""
    @staticmethod
    def get_gpt_generation(keyword: str, lang: str = "",mode: str = "body") -> Optional[str]:
        gpt = random.choice(gpts)
        role = f"ä½ æ˜¯ä¸€ä¸ªå…·æœ‰ä¸°å¯Œè¡Œä¸šçŸ¥è¯†{'ï¼Œæ·±è°™ä¸­å›½ã€Šå¹¿å‘Šæ³•ã€‹' if lang !='è‹±æ–‡' else ''}çš„èµ„æ·±{lang}æ–‡æ¡ˆç¼–è¾‘"
        if mode == "body":
            prompts = f"""è¯·ä»¥<{keyword}>ä¸ºæ ‡é¢˜ï¼Œå¹¶ç»“åˆè¡Œä¸šç‰¹ç‚¹å†™ä¸€ç¯‡{lang}çˆ†æ¬¾ç§‘æ™®æ€§æ–‡ç« 

è¦æ±‚ï¼š
- é‡‡ç”¨æ–‡æ¡ˆåˆ›ä½œé»„é‡‘ä¸‰ç§’åŸåˆ™ã€‚
- è¦æ±‚ç«™åœ¨ç”¨æˆ·è§’åº¦æ€è€ƒå’Œè§£è¯»ï¼Œç›´å‡»ç”¨æˆ·ç—›ç‚¹ï¼Œå…±æƒ…ç”¨æˆ·æƒ…ç»ªï¼Œå¹¶ä¸”å¼•å¯¼ç”¨æˆ·å’¨è¯¢å’ŒæŒç»­å…³æ³¨ã€‚
- æ’ç‰ˆéœ€è¦è€ƒè™‘é˜…è¯»ä½“éªŒï¼Œåˆç†å®‰æ’é‡ç‚¹ä¿¡æ¯è¯è¯­æˆ–è€…æ®µè½é«˜äº®ï¼Œæ¯è¡Œéƒ½éœ€è¦é—´éš”ä¸€ä¸ªç©ºè¡Œï¼ˆä»£ç é™¤å¤–ï¼‰ï¼Œåˆ†å‰²ç¬¦ç»Ÿä¸€ä½¿ç”¨20ä¸ªè¿ç»­ç ´æŠ˜å·ä»£æ›¿ã€‚
- æ­£æ–‡å¼•ç”¨å¤„ç”¨ä¸Šæ ‡æ ‡æ³¨ï¼Œæ–‡æœ«æŒ‰APAæ ¼å¼åˆ—å°¾æ³¨ï¼ˆä½œè€…ï¼Œå¹´ä»½ï¼Œæ ‡é¢˜ï¼ŒæœŸåˆŠï¼ŒDOIï¼‰ï¼Œä¸å¾—è™šæ„å°¾æ³¨å†…å®¹ã€‚
- åœ¨ä¸ç ´åé˜…è¯»ä½“éªŒçš„æƒ…å†µä¸‹åœ¨åˆé€‚ä½ç½®ï¼ˆæ®µè½ä¹‹å¤–ï¼šä¸å¾—ç ´åæ®µè½ç»“æ„ï¼‰å®‰æ’æ’å…¥ â€œ[æ–‡å†…æ’å›¾]â€ è¿›è¡Œå ä½ï¼Œä»¥ä¸°å¯Œæ–‡ç« å†…å®¹ã€‚
- ä¸è¦å‡ºç°ä¸å†…å®¹æ— å…³çš„è§£é‡Šæ€§è¯­å¥å’Œæ–‡ç« æç¤ºï¼šä¸è¦è§£é‡Šâ€œæˆ‘æ­£åœ¨ç”Ÿæˆæ–‡ç« â€ä¹‹ç±»çš„å¼•å¯¼è¯­ï¼›ä¸å¾—å‡ºç° "æ ‡é¢˜ï¼š" æˆ–ç±»ä¼¼æç¤ºè¯­ã€‚

æ­¤å¤–ï¼Œæ–°å†™çš„æ–‡ç« éœ€è¦å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š
- æ–‡ç« ç»“æ„æ¸…æ™°ï¼Œæ®µè½è¿‡æ¸¡è‡ªç„¶ï¼Œé¿å¼€å†·åƒ»è¯æ±‡ï¼Œåˆ å‡æ–‡ç« æœ«å°¾æ€»ç»“/ç»“è®º/å±•æœ›éƒ¨åˆ†ï¼Œè®©å†…å®¹ç®€æ´æµç•…ï¼Œä¾¿äºè½»æ¾æŠŠæ¡æ€è·¯ä¸æ„å›¾ã€‚
- æ›¿æ¢æˆ–å‡å°‘ç»“æ„åŒ–è¿æ¥è¯ï¼ˆå¦‚ï¼šé¦–å…ˆã€å…¶æ¬¡ã€ç„¶åã€å†æ¬¡ã€æœ€åã€æ€»ä¹‹ã€æ€»è€Œè¨€ä¹‹ã€ç„¶è€Œã€å› æ­¤ã€å¦å¤–ã€æ­¤å¤–ï¼‰ï¼Œæ”¹ç”¨æ›´åŸºç¡€ã€å¸¸ç”¨ã€ç”šè‡³å£è¯­åŒ–çš„è¡¨è¾¾ï¼Œä»¥å¢åŠ è‡ªç„¶åº¦å’Œå¯è¯»æ€§ã€‚
- å†…å®¹å……å®ï¼Œè®ºæ®å……åˆ†ï¼Œæ”¯æ’‘è§‚ç‚¹çš„åŒæ—¶å±•ç°ç‹¬ç‰¹è§†è§’ï¼›è¾…ä»¥æ¡ˆä¾‹ä¸æ•°æ®ï¼Œæå‡å¯ä¿¡åº¦ä¸è¯´æœåŠ›ï¼ˆè°·æ­Œç®—æ³•EEATåŸåˆ™ï¼‰ã€‚
- é£æ ¼é€‚é…ï¼Œè¯­è¨€è¦ç´ ï¼ˆç”¨è¯/è¯­æ°”/å¥å¼/ç»“æ„ï¼‰è´´åˆè¯»è€…éœ€æ±‚ï¼›èå…¥ä¸ªäººè§†è§’ä¸æƒ…æ„Ÿï¼Œå¢å¼ºæ–‡ç« ä¸ªæ€§ä¸æ¸©åº¦ã€‚
- èåˆBERTè¯­ä¹‰å»ºæ¨¡ã€TF-IDFå‘é‡åˆ†æã€BM25ç›¸å…³æ€§æ’åºã€ä¸‰å…ƒç»„çŸ¥è¯†å›¾è°±ä¸LDAä¸»é¢˜æ¨¡å‹ï¼Œå…¨é¢æå‡SEOçš„è¯­ä¹‰ç†è§£ã€å…³é”®è¯ä¼˜åŒ–åŠå†…å®¹å…³è”æ€§ã€‚
- è§„é¿ã€Šå¹¿å‘Šæ³•ã€‹ç¦ç”¨è¯ï¼šç¦ç”¨â€œæœ€â€â€œå”¯ä¸€â€ç­‰ç»å¯¹è¯ï¼›æ•ˆæœæè¿°åŠ â€œå¯èƒ½â€â€œéƒ¨åˆ†ç”¨æˆ·åé¦ˆâ€ç­‰é™å®šè¯ï¼›åŒ»ç–—/é£Ÿå“ç±»ç¦ç”¨â€œæ²»ç–—â€â€œæ²»æ„ˆâ€ï¼Œæ”¹ç”¨â€œè¾…åŠ©â€â€œæŠ¤ç†â€ï¼›æ•°æ®æ ‡æ³¨æ¥æºï¼ˆå¦‚â€œå†…éƒ¨è°ƒç ”æ˜¾ç¤ºâ€¦â€ï¼‰ã€‚
- ä¸å¾—å¼•ç”¨æˆ–è€…ä½¿ç”¨è™šå‡ç½‘å€ï¼ˆå¦‚ï¼šexample.comï¼‰å’Œè™šå‡æ•°æ®ï¼ˆå¦‚ï¼šå“ç‰Œã€äº§å“ã€å…¬å¸ã€ç»„ç»‡ã€ç½‘ç«™ã€æœºæ„ã€äººå‘˜ã€äº‹ä»¶ã€åœ°ç‚¹ã€æ—¶é—´ã€æ•°é‡ã€é‡‘é¢ã€ç™¾åˆ†æ¯”ã€æ¯”ä¾‹ã€æ¦‚ç‡ç­‰ã€‚
- å†™æ³•æŒ‡å—ï¼šæœç»æ ‡é¢˜å…šï¼ˆå†…å®¹éœ€ä¸æ ‡é¢˜åŒ¹é…ï¼‰ï¼Œé¿å…å…³é”®è¯å †ç Œï¼ˆé˜²æ­¢æœºæ¢°æ„Ÿï¼‰ï¼Œæ‹’ç»è‡ªå—¨ï¼ˆæµ‹è¯•ä¸‰äººæ³•åˆ™ï¼šä¸‰äººè¶…3ç§’çŠ¹è±«åˆ™é‡å†™ï¼‰ã€‚èšç„¦æ ‡é¢˜å’Œå…³é”®è¯ï¼Œåˆ‡å¿Œä¸è¦åé¢˜ã€è·‘é¢˜ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸Šè¦æ±‚ï¼Œåªéœ€ä»¥Markdownæ ¼å¼è¾“å‡ºæ­£æ–‡å†…å®¹ï¼Œä¸å†éœ€è¦å†è¾“å‡ºæ ‡é¢˜ä»¥åŠå…¶ä»–è§£é‡Šã€‚
"""
        elif mode == "title":
            prompts = f"""è¯·ä»¥<{keyword}>ä¸ºå…³é”®è¯ï¼Œå‚è€ƒä¸‹æ–¹10ç§æ ‡é¢˜å†™ä½œæ–¹æ³•ï¼Œå¹¶ç»“åˆè¡Œä¸šç‰¹ç‚¹å†™æ³•æ‹Ÿä¸€ä¸ª{lang}çˆ†æ¬¾æ ‡é¢˜

è¦æ±‚ï¼š
- æœç»ä½¿ç”¨â€œï¼šâ€åˆ†å‰²æ ‡é¢˜ï¼Œæ ‡é¢˜æœ«å°¾ä¸éœ€è¦å¥å·ï¼›
- ä½¿ç”¨å£è¯­åŒ–è¡¨è¾¾ï¼Œé¿å…æœºæ¢°æ„Ÿï¼›
- èƒ½æ¿€å‘æƒ…ç»ªå…±é¸£ï¼ˆå¦‚å¥½å¥‡ã€ç„¦è™‘ã€æƒŠå–œã€æ„¤æ€’ã€æƒŠæï¼‰ï¼›
- åˆ¶é€ è®¤çŸ¥åå·®æˆ–æ‚¬å¿µï¼Œå¼•å¯¼ç‚¹å‡»ï¼›
- ä¸éœ€è¦ä»»ä½•è§£é‡Šæ€§è¯­å¥æˆ–å‰¯æ ‡é¢˜ï¼›
- ç›´æ¥è¿”å›æ ‡é¢˜æ–‡å­—ï¼Œä¸è¦åŠ Markdownæ ¼å¼æˆ–æ ‡ç­¾ã€‚

ä»¥ä¸‹æ˜¯10ç§æ ‡é¢˜å†™ä½œæ–¹æ³•ä¾›å‚è€ƒï¼š
ã€ç¬¬ä¸€æ‹›ã€‘æ•°å­—æ³•è¿›é˜¶ç‰ˆï¼šæœˆè–ª5åƒåˆ°5ä¸‡ï¼Œæˆ‘ç”¨äº†237å¤©
ã€ç¬¬äºŒæ‹›ã€‘æ‚¬å¿µè®¾è®¡æ³•ï¼šé¢†å¯¼åŠå¤œå‘å¾®ä¿¡ï¼Œæˆ‘æ•´æ™šæ²¡ç¡ç€
ã€ç¬¬ä¸‰æ‹›ã€‘ç—›ç‚¹åœºæ™¯åŒ–ï¼šå­©å­è€ƒä¸ä¸Šé‡ç‚¹é«˜ä¸­ï¼Ÿè¿™5ä¸ªè‡ªæ•‘æ–¹æ¡ˆç°åœ¨çœ‹è¿˜æ¥å¾—åŠ
ã€ç¬¬å››æ‹›ã€‘æƒ…ç»ªè¿‡å±±è½¦ï¼šåŒäº‹æ®ä¸ºå·±æœ‰â†’åå‡»é¼“æŒï¼›è¢«æˆ¿ä¸œèµ¶å‡ºâ†’ä½æœˆç§Ÿ500è±ªå®…
ã€ç¬¬äº”æ‹›ã€‘çƒ­ç‚¹é™ç»´æœ¯ï¼šè°·çˆ±å‡Œæ¯å¤©ç¡10å°æ—¶ï¼Ÿæ‰“å·¥äººå­¦ä¸‰ç‚¹å°±å¤Ÿäº†
ã€ç¬¬å…­æ‹›ã€‘è®¤çŸ¥åå·®æœ¯ï¼šè¶Šçœé’±çš„äººè¶Šç©·ï¼Œæ‹¼å‘½å·¥ä½œçš„äººå‡ä¸äº†èŒ
ã€ç¬¬ä¸ƒæ‹›ã€‘çµé­‚æ‹·é—®æ³•ï¼šä¸ºä»€ä¹ˆä½ å­¦äº†100ä¸ªå†™ä½œè¯¾è¿˜æ˜¯å†™ä¸å¥½æ–‡æ¡ˆï¼Ÿ
ã€ç¬¬å…«æ‹›ã€‘å¼ºåŠ¿æŒ‡ä»¤æ³•ï¼šæ”¶è—ï¼è¿™20ä¸ªExcelå…¬å¼çœ80%å·¥ä½œé‡
ã€ç¬¬ä¹æ‹›ã€‘ç¬¦å·èŠ‚å¥æœ¯ï¼šæœˆå…¥3W+ï¼é€‚åˆæ‡’äººçš„æé’±å‰¯ä¸šï¼ˆé™„æ¸ é“ï¼‰
ã€ç¬¬åæ‹›ã€‘æ¯›å­”çº§åœºæ™¯ï¼šè¹²é©¬æ¡¶åˆ·æ‰‹æœºæ—¶é¡ºæ‰‹å°±èƒ½åšçš„5ä¸ªæé’±å‰¯ä¸š

è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚ç”Ÿæˆä¸€ä¸ªæ ‡é¢˜ï¼Œåªè¾“å‡ºæ ‡é¢˜å†…å®¹ï¼Œä¸éœ€è¦å…¶ä»–è§£é‡Šã€æ ‡ç­¾æˆ–æ ¼å¼ã€‚
""" 
        elif mode == "excerpt":
            prompts = f"""è¯·æ ¹æ®æˆ‘æä¾›çš„æ–‡ç« å†…å®¹å¹¶ç»“åˆæœç´¢å¼•æ“è§„åˆ™å¯¹æ–‡ç« åšä¸€ä¸ª100å­—å·¦å³{lang}ç®€å•æ¦‚è¿°ï¼Œä»¥çº¯æ–‡æœ¬è¾“å‡ºæ¦‚è¿°å†…å®¹ï¼Œæ— éœ€åŸæ–‡å’Œå…¶ä»–ï¼Œä»¥ä¸‹æ˜¯æ–‡ç« å†…å®¹ï¼š\n{keyword}"""
        elif mode == "tags":
            prompts = f"""è¯·æ ¹æ®æˆ‘æä¾›çš„æ–‡ç« å†…å®¹å¹¶ç»“åˆæœç´¢å¼•æ“è§„åˆ™æå–å‡º5ä¸ªåˆé€‚çš„å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä»¥çº¯æ–‡æœ¬è¾“å‡ºå…³é”®è¯å†…å®¹ï¼Œæ— éœ€åŸæ–‡å’Œå…¶ä»–ï¼Œä»¥ä¸‹æ˜¯æ–‡ç« å†…å®¹ï¼š\n{keyword}"""
        else:
            prompts = ''
        client = OpenAI(
            api_key=random.choice(gpt["apikey"]), 
            base_url=gpt["baseurl"],
            http_client=httpx.Client(verify=False)
        )
        try:
            completion = client.chat.completions.create(
                model=random.choice(gpt["models"]),
                messages=[
                    {"role": "system","content": role},
                    {"role": "user","content": prompts}
                ],
                n=1,                    # è¾“å‡ºçš„æ¡æ•°
                temperature=0.5,        # è¾“å‡ºçš„éšæœºæ€§ï¼Œå€¼è¶Šä½è¾“å‡ºè¶Šç¡®å®š
                top_p=0.8,              # è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œå€¼è¶Šä½è¾“å‡ºè¶Šé›†ä¸­
                max_tokens=4096,        # æ§åˆ¶ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡
                frequency_penalty=0.5,  # å‡å°‘é‡å¤å†…å®¹çš„ç”Ÿæˆ
                presence_penalty=0.5    # é¼“åŠ±æ¨¡å‹å¼•å…¥æ–°å†…å®¹
            )
            content = completion.choices[0].message.content
            return re.sub(r'<think>.*</think>', '', content.strip(), flags=re.DOTALL)
        except Exception as e:
            logging.error(f"{str(e)}")
            return None
        

    """æ•´åˆæˆå‘å¸ƒçš„æ•°æ®ç»“æ„"""
    @staticmethod
    def get_article_data(platform: Dict) -> List[Dict]:
        # æ–‡ç« å†…å®¹äºŒæ¬¡ç²¾ç»†åŒ–å¤„ç†
        articles = []
        # è·å–æ‰€æœ‰å…³é”®è¯æˆ–è€…æ ‡é¢˜åˆ—è¡¨ï¼Œæ ¹æ®æ–‡ä»¶åï¼škeywords.txt / titles.txt
        # å½“å‰æ˜¯å…³é”®è¯æ¨¡å¼ï¼Œè¿˜æ˜¯æ ‡é¢˜æ¨¡å¼
        mode = 'keywords' if 'keywords' in platform.get('keys') else 'titles'
        if '.txt' not in platform.get('keys'):
            keys = list(set(platform.get('keys')))
        elif (key_path := WorkDIR / platform.get('keys')).exists():
            with key_path.open('r', encoding='utf-8') as _key:
                keys = list(set([s for l in _key if (s := re.sub(r'[\n\ufeff]', '', l)) and len(s) > 1]))
        else:
            return []
        # ç”Ÿæˆæ‰€éœ€æ–‡ç« æ•°æ®
        for _ in range(platform.get('number', 1)): 
            key = random.choice(keys)                       # éšæœºé€‰æ‹©ä¸€ä¸ªåŸºç¡€å…³é”®è¯ 
            # æ–‡ç« æ ‡é¢˜ç”Ÿæˆ         
            _title = ''
            if mode == "titles":                            # çº¯æ ‡é¢˜ï¼Œä¸éœ€è¦ç”Ÿæˆæ ‡é¢˜
                _title = key
            else:                                           # çº¯å…³é”®è¯ï¼Œéœ€è¦ç”Ÿæˆæ ‡é¢˜
                for _t in range(3):
                    _tit = Extensions.get_gpt_generation(keyword=key, lang=platform.get('lang'), mode="title")
                    if _tit and not (_tit.startswith('<p>') or _tit.startswith('<h3>') or ' error' in _tit):
                        _title = re.sub(r'[\n<>"\'ã€Šã€‹{}ã€ã€‘ã€Œã€â€”â€”ã€‚]|&nbsp;|&lt;|&gt;|boxed', '', _tit).strip(' \n\r"\'')
                        zh_title_len = len(re.findall(r'[\u4e00-\u9fff]', _title))
                        en_title_len = len(re.findall(r"[A-Za-z'-]+", _title))
                        if not ((5 < zh_title_len < 30) or (5 < en_title_len < 30)):
                            logging.error("æ–‡ç« æ ‡é¢˜ ã€å­—æ•°ã€‘ ä¸ç¬¦åˆè¦æ±‚")
                            continue
                        break
                    else:
                        logging.error(f"æ–‡ç« æ ‡é¢˜ ã€å†…å®¹ã€‘ ä¸ç¬¦åˆè¦æ±‚")
                        continue
            logging.info(f"{_title}, {len(_title)}")
            
            # æ–‡ç« å†…å®¹è§„åˆ™
            for _t in range(3):
                _content = Extensions.get_gpt_generation(keyword=_title, lang=platform.get('lang'), mode="body")
                if _content and len(_content) > 100:
                    zh_content_len = len(re.findall(r'[\u4e00-\u9fff]', _content))
                    en_content_len = len(re.findall(r"[A-Za-z'-]+", _content))
                    if not (zh_content_len > 300 or en_content_len > 300):
                        logging.error(f"æ–‡ç« è¯¦æƒ… ã€å­—æ•°ã€‘ ä¸ç¬¦åˆè¦æ±‚")
                        continue
                    # æ›¿æ¢åˆ†éš”ç¬¦ï¼šç»Ÿä¸€åˆå¹¶ä¸º20ä¸ªç¬¦å·
                    _content = re.sub(
                        r'^([=-]{3,})[\r\n]+', 
                        lambda m: '='*20 + '\n' if m.group(1).startswith('=') else '-'*20 + '\n',
                        _content,
                        flags=re.MULTILINE
                    )
                    # å°è¯•å¤šç§æ–¹å¼åŒ¹é…å¹¶ç§»é™¤æ ‡é¢˜
                    # æƒ…å†µ1ï¼šç¬¬ä¸€è¡Œä¸ºæ ‡é¢˜(æˆ–è€…å¸¦#å·) -> ç§»é™¤ç¬¬ä¸€è¡Œ
                    # æƒ…å†µ2ï¼šæ ‡é¢˜è¢«æ‹†åˆ†ä¸ºä¸¤è¡Œï¼Œå¦‚ â€œç¬¬ä¸€éƒ¨åˆ†\nç¬¬äºŒéƒ¨åˆ†â€ æˆ–è€… â€œç¬¬ä¸€éƒ¨åˆ†\nâ€”â€”ç¬¬äºŒéƒ¨åˆ†â€ -> ç§»é™¤å‰ä¸¤è¡Œ
                    # æœ€åå¤„ç†å‰©ä¸‹çš„å†…å®¹ï¼šåˆ é™¤ä»¥ 'â€”â€”' å¼€å¤´çš„é¦–è¡Œï¼ˆå¦‚æœ‰ï¼‰
                    lines = _content.split('\n')
                    if lines and (lines[0] == _title or re.fullmatch(rf'#\s+{re.escape(_title)}', lines[0])):
                        lines = '\n'.join(lines[1:]) 
                    elif len(lines) >= 2 and ((lines[0] + lines[1]).strip() == _title or re.match(rf'^â€”+\s+{re.escape(_title.split()[-1])}$', lines[1])):
                        lines = '\n'.join(lines[2:]) 
                    else:
                        pass
                    if lines and lines[0].startswith('â€”â€”'): _content = '\n'.join(lines[1:])
                    break
                else:
                    logging.error(f"æ–‡ç« è¯¦æƒ… ã€å†…å®¹ã€‘ ä¸ç¬¦åˆè¦æ±‚")
                    continue
            
            # æå–æ–‡ç« æ ‡ç­¾
            if not platform.get('aitags'):
                if '.txt' not in platform.get('reqkeys'):
                    reqkeys = list(set(platform.get('reqkeys')))
                elif (reqkeys_path := WorkDIR / platform.get('reqkeys')).exists():
                    with reqkeys_path.open('r', encoding='utf-8') as req_key:
                        reqkeys = list(set([s for l in req_key if (s := re.sub(r'[\n\ufeff]', '', l)) and len(s) > 1]))
                else:
                    reqkeys = keys if mode == 'keywords' else []
                _tags = Extensions.extract_article_keywords(_content, reqkeys)[:5]
            else:
                _ts = Extensions.get_gpt_generation(keyword=_content, lang=platform.get('lang'), mode="tags")
                _tags = [x.strip() for x in re.split(r'[,\u3001]+', _ts) if x.strip()]
            
            # æå–æ–‡ç« æ‘˜è¦
            _excerpt = Extensions.get_gpt_generation(keyword=_content, lang=platform.get('lang'), mode="excerpt")
            
            # æå–æ–‡ç« å›¾ç‰‡
            _img = f"""![{key}](https://image.pollinations.ai/prompt/{key}?width=1200&height=900&enhance=true&private=true&nologo=true&safe=true&model=flux "{key}")"""
            _content = _content.replace('[æ–‡å†…æ’å›¾]', _img)
            _pictures = list(re.compile(r'!\[.*?\]\(\s*([^\s)]+)(?:\s+|\))').findall(_content))
           
            # å¤„ç†æ–‡ç« å†…å®¹
            _content = Extensions.handle_article_content(_title, _content, platform)

            # æ„å»ºæ–‡ç« æ•°æ®å­—å…¸
            article = {
                "title": _title,
                "keyword": key,
                "content": _content,
                "tags": _tags if _tags else [key],
                "excerpt": _excerpt,
                "date": time.strftime("%Y-%m-%dT%H:%M:%S%z", time.localtime()),
                "pictures": _pictures,
                "cover": _pictures[:1] if _pictures else '',
            }
            # åˆ¤æ–­æ–‡ç« æ˜¯å¦ä¸ºç©º(æ‰€æœ‰å­—æ®µ)
            # if not any(v == "" or v is None or (hasattr(v, '__len__') and len(v) == 0) for v in article.values()):
            # åˆ¤æ–­æ–‡ç« æ˜¯å¦ä¸ºç©º(æ’é™¤åˆ—è¡¨)
            if not any(v == "" or v is None or (not isinstance(v, list) and hasattr(v, "__len__") and len(v) == 0) for v in article.values()):
                # æ·»åŠ åˆ°articlesåˆ—è¡¨
                articles.append(article)
                # å¯¼å‡ºåˆ°Markdownæ–‡ä»¶
                Extensions.export_to_markdown(platform, article)
        return articles

    
    """æ•´åˆå…¶ä»–ä¿¡æ¯åˆ°æ–‡ç« ä¸­"""
    @staticmethod
    def handle_article_content(title: str, content: str, platform: Dict) -> str:
        # 1. å®šä¹‰ Markdown ç‰¹å¾çš„æ­£åˆ™è¡¨è¾¾å¼
        markdown_patterns = [
            r'^#{1,6}\s+',                    # æ ‡é¢˜ï¼ˆå¦‚ #, ##ï¼‰
            r'^[-*]\s+',                       # æ— åºåˆ—è¡¨ï¼ˆ- æˆ– * å¼€å¤´ï¼‰
            r'^\d+\.\s+',                      # æœ‰åºåˆ—è¡¨ï¼ˆæ•°å­—. å¼€å¤´ï¼‰
            r'\*\*.*?\*\*|\*.*?\*',            # ç²—ä½“ (**text**) æˆ–æ–œä½“ (*text*)
            r'!\[.*?\]\(.*?\)|\[.*?\]\(.*?\)', # å›¾ç‰‡æˆ–é“¾æ¥
            r'^```.*?^```',                    # ä»£ç å—ï¼ˆå¤šè¡Œï¼‰
            r'`.*?`',                          # è¡Œå†…ä»£ç ï¼ˆ`text`ï¼‰
            r'^>\s+',                          # å¼•ç”¨ï¼ˆ> å¼€å¤´ï¼‰
            r'^\|.*?\|',                       # è¡¨æ ¼ï¼ˆ|...|ï¼‰
            r'^[=-]{3,}\s*$',                  # åˆ†å‰²çº¿ï¼ˆ--- æˆ– ===ï¼‰
            r'^\s*[*-]{3,}\s*$',               # å•ç‹¬çš„åˆ†å‰²çº¿ï¼ˆ---, ***ï¼‰
        ]
        # 2. ç»Ÿè®¡æœ‰æ•ˆç‰¹å¾æ•°é‡
        count = 0
        # 3. é¢„å¤„ç†ï¼šç§»é™¤ä»£ç å—å’Œè¡Œå†…ä»£ç 
        _text = re.sub(r'(?s)`.*?`', '', re.sub(r'(?s)```.*?```|~~~.*?~~~', '', content))
        lines = list(map(str.strip, _text.split('\n')))
        for line in lines:
            if not line: continue
            for pattern in markdown_patterns:
                if re.search(pattern, line, flags=re.MULTILINE | re.DOTALL):
                    count += 1
                    break
        # 4. åˆ¤å®šæ˜¯å¦æ˜¯Markdownå†…å®¹
        if count >= 2:
            # ä½¿ç”¨åˆ†éš”ç¬¦åˆ†å‰²æ–‡æœ¬ï¼Œå»é™¤æ ‡é¢˜ï¼Œåªå–ååŠéƒ¨åˆ†çº¯æ–‡ç« å†…å®¹
            # content = re.sub(r'^(={3,}[\r\n]+)', '====================\n', content, flags=re.MULTILINE)
            # content = re.sub(r'^(-{3,}[\r\n]+)', '--------------------\n', content, flags=re.MULTILINE)
            # æ›¿æ¢åˆ†éš”ç¬¦ï¼šç»Ÿä¸€åˆå¹¶ä¸º20ä¸ªç¬¦å·
            content = re.sub(
                r'^([=-]{3,})[\r\n]+', 
                lambda m: '='*20 + '\n' if m.group(1).startswith('=') else '-'*20 + '\n',
                content,
                flags=re.MULTILINE
            )
            # åˆ†éš”æ ‡é¢˜ä¸å†…å®¹
            parts1 = content.split(f"====================\n", 1)  # æœ€å¤šåˆ†å‰²ä¸€æ¬¡
            parts2 = content.split(f"--------------------\n", 1)  # æœ€å¤šåˆ†å‰²ä¸€æ¬¡
            if len(parts1) > 1 and parts1[0].strip(' \n\r"\'').replace('# ', '', 1) == title:
                content = parts1[1].strip()
            elif len(parts2) > 1 and parts2[0].strip(' \n\r"\'').replace('# ', '', 1) == title:
                content = parts2[1].strip()
            else:
                lines = [line for line in content.strip(' \n\r"\'').splitlines() if line.strip()]
                if lines[0].strip(' \n\r"\'') == title:
                    content = content.replace(lines[0], '', 1).strip()

        # ç§»é™¤æ‰€æœ‰çº§åˆ«çš„æ€»ç»“ç±»æ ‡é¢˜ï¼ˆå¦‚ï¼šç»“è¯­ã€æ€»ç»“ã€å°ç»“ã€ç»“è®ºï¼‰
        zjbt = r'^(?:#{2,6})\s*(?:ç»“è¯­|æ€»ç»“|å°ç»“|ç»“è®º|æ€»è€Œè¨€ä¹‹|ç»¼ä¸Šæ‰€è¿°|æœªæ¥å±•æœ›|å±•æœ›æœªæ¥|å†™åœ¨æœ€å)\s*$\n?'
        wmjz = r'(?:[â°Â¹Â²Â³â´âµâ¶â·â¸â¹]+\.)?\s*(?:æ ‡é¢˜|é¢˜ç›®|æ–‡çŒ®|å‚è€ƒ)?(è„šæ³¨|æ³¨è„š)ï¼š["â€œ]([^ï¼Œ]+)ï¼Œ(\d{4})ï¼Œ([^ï¼Œ]+)ï¼Œ([^ï¼Œ]+)ï¼Œ(10\.\d{4}\/[^"â€]+)["â€]' 
        content = re.sub(fr'({zjbt}|{wmjz})', '', content, flags=re.MULTILINE | re.IGNORECASE)
        desc = r'^.*?ã€?(?:æœ¬æ–‡|æ–‡ç« |æœ¬ç¯‡|å…¨æ–‡|å‰è¨€)\s*[ï¼Œ,]?\s*(?:ç®€ä»‹|æ‘˜è¦|æ¦‚è¿°|å¯¼è¯»|æè¿°|å¼•è¨€)ã€‘?[ï¼š:]?'
        content = markdown.markdown(
            re.sub(desc,'', content, flags=re.DOTALL | re.MULTILINE), # å»æ‰æœºæ¢°å¼å¼€å¤´
            output_format='html',
            extensions=markdown_extensions,
            extension_configs=markdown_extconfigs,
            safe_mode='escape',
            enable_attributes=True,
            linkify=True,
            tab_length=4,
            lazy_ol=False,
            toc=True,
            toc_depth=4,
            toc_title=u'æ–‡ç« ç›®å½•',
            toc_nested=True,
            toc_list_type='ul',
            toc_list_item_class='toc-list-item',
            toc_list_class='toc-list',
            toc_header_id='toc',
            toc_anchor_title=u'è·³è½¬è‡³æ–‡ç« ç›®å½•',
            toc_anchor_title_class='toc-anchor-title'
        )
        # åŒ¹é…æ‰€æœ‰<h3>ã€<p>æ ‡ç­¾ï¼Œè®¾ç½®æ ·å¼
        content = re.sub(r'<p>', platform.get('phtag'), content)
        content = re.sub(r'<h3>', platform.get('hrtag'), content)
        # æ–‡ç« å†å¤„ç†ï¼ˆç¬¦åˆå¹³å°éœ€è¦ï¼‰æœ‰BUG
        # content  = f"""
        #     {platform.get('service_ad')}
        #     {content}
        #     {platform.get('bottom_ad_top')}
        #     {platform.get('contact')}
        #     {platform.get('bottom_ad_bottom')}
        #     {platform.get("followme")}
        # """
        return content


    # ç”ŸæˆCMSæ–‡ç« Dict
    def export_to_markdown(platform: Dict, data: Dict) -> Optional[bool]:
        # ç”ŸæˆFront Matter
        # å°†ä¸­æ–‡éƒ¨åˆ†è½¬æ¢ä¸ºæ‹¼éŸ³ï¼ˆä¿ç•™è‹±æ–‡å’Œæ•°å­—ï¼‰
        cn_lang = any(
            (u'\u4e00' <= char <= u'\u9fa5') or
            (u'\u3400' <= char <= u'\u4DBF') or
            (u'\U00020000' <= char <= u'\U0002A6DF')
            for char in data.get('title', '')
        )
        if cn_lang:
            _title = '-'.join(lazy_pinyin(data.get('title', ''), style=Style.NORMAL, strict=False))
        else:
            _title = data.get('title', '')
        urlname = slugify(
            _title,
            separator='-',
            lowercase=True,
            regex_pattern=None,
            word_boundary=True,
            stopwords=[],
            replacements=[]
        )
        # åˆ›å»º TOML æ–‡æ¡£å¯¹è±¡
        doc = tomlkit.document()
        # åŸºç¡€å­—æ®µ
        doc["title"] = data.get('title')
        doc["date"] = data.get('date')
        doc["tags"] = data['tags'][:5]
        doc["keywords"] = data['tags'][:5]
        doc["description"] = data.get('excerpt') or ""
        doc["categories"] = platform.get('categories')  or []
        doc["author"] = platform.get('author') or "lopins"
        doc["cover"] = data.get('cover') or ""
        doc["pictures"] = data.get('pictures') or []
        doc["hiddenFromHomePage"] = False
        doc["readingTime"] = True
        doc["hideComments"] = True
        doc["isCJKLanguage"] = True
        doc["slug"] = urlname
        # å¤„ç†æ‰©å±•å­—æ®µæ·»åŠ åˆ°æ–‡æ¡£
        _extras = {}
        for k, v in data.get('extras', {}).items():
            if isinstance(v, str):
                v = v.strip().strip('(ï¼ˆï¼‰)')
                p = int(v) if v.isdigit() else v
            else:
                p = v
            _extras[k] = p if isinstance(p, int) else f'{json.dumps(p,ensure_ascii=False)[1:-1]}'
        for key, value in _extras.items():
            doc[key] = value
        # æ–‡ç« çŠ¶æ€
        doc["draft"] = False
        # åºåˆ—åŒ–ä¸º TOML å­—ç¬¦ä¸²ï¼ˆä¿ç•™æ ¼å¼ï¼‰
        front_matter_block = f"+++\n{tomlkit.dumps(doc).strip()}\n+++"

        if platform.get('cms') == 'hexo':
            content = front_matter_block.strip()[3:-3].strip()  # ç›´æ¥å»é™¤+++åˆ†éš”ç¬¦
            lines = content.split('\n')
            _yaml = []
            for line in lines:
                line = line.strip()
                if not line: continue
                key, val = line.split('=', 1)
                key = key.strip()
                val = val.strip()
                if val.startswith('[') and val.endswith(']'):
                    # å¤„ç†åˆ—è¡¨é¡¹ï¼ˆç§»é™¤æ–¹æ‹¬å·å¹¶åˆ†å‰²ï¼‰
                    items = [i.strip().strip("'\"") for i in val[1:-1].split(',')]
                    _yaml.append(f"{key}:")
                    _yaml.extend(f"  - {item}" for item in items if item)
                elif val in ('true', 'false'):
                    _yaml.append(f"{key}: {val}")
                else:
                    _yaml.append(f"{key}: {val.strip('\"')}")
            front_matter_block = f"---\n{'\n'.join(_yaml)}\n---"
        try:
            doc_name = f"{re.sub(r'\D', '', data.get('date'))}-{uuid.uuid4()}.md"
            file_path = Path(WorkDIR) / "articles" / doc_name
            file_path.parent.mkdir(parents=True, exist_ok=True) 
            with file_path.open("w", encoding="utf-8") as f:
                f.write(f"{front_matter_block}\n\n{markdownify(data['content'])}")
            return True if file_path.exists() and file_path.stat().st_size > 0 else False
        except Exception as e:
            logging.error(f"ä¿å­˜æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
            return False    


if __name__ == "__main__":
    # è·å–æ‰€æœ‰å¤§æ¨¡å‹é…ç½®
    gpts = Extensions.get_gpt_config(gpts=config.get('gpts'))
    # logging.info(f"è·å–æ‰€æœ‰å¯ç”¨GPTé…ç½®ï¼š{gpts[0]["models"]}")
    if gpts: 
        for wechat in config.get('mps'):
            WeChatMP(wechat, gpts).main()
