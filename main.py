#-*- coding: UTF-8 -*-
__author__ = "https://github.com/lopinx"
# å¯¼å‡ºåŒ…ï¼š uv pip freeze | uv pip compile - -o requirements.txt
# =========================================================================================================================
# pip install httpx[http2,http3] keybert scikit-learn jieba nltk rank-bm25 fuzzywuzzy python-Levenshtein markdown pygments pymdown-extensions markdownify openai pandas
# ==========================================================================================================================
import json
import logging
import random
import re
import time
import uuid
from collections import defaultdict, Counter, OrderedDict
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import httpx
import jieba
import jieba.analyse
import markdown  # markdownè½¬html
import nltk
import numpy as np
from fuzzywuzzy import process
from keybert import KeyBERT
from markdownify import markdownify  # htmlè½¬markdown
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
# from nltk.util import everygrams
from openai import OpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from rank_bm25 import BM25Okapi

#  ##########################################################################################################################
# å½“å‰å·¥ä½œç›®å½•,é…ç½®æ–‡ä»¶
WorkDIR = Path(__file__).resolve().parent
config = json.load(open(WorkDIR/"config.json", 'r', encoding='utf-8'))
# ä¸‹è½½åˆ†è¯åº“æ•°æ®ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ï¼‰
try:
    nltk.corpus.stopwords.words()
except LookupError:
    nltk.download('stopwords')
try:
    nltk.sent_tokenize("Test sentence")
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')
# markdownåº“æ‰©å±•
markdown_extensions = [
    'fenced_code',                                          # ä»£ç å—ï¼ˆä¸‰ä¸ªåå¼•å·åŒ…è£¹ï¼‰
    'codehilite',                                           # ä»£ç é«˜äº®ï¼ˆéœ€å®‰è£… pygmentsï¼‰
    'tables',                                               # è¡¨æ ¼æ”¯æŒ
    'footnotes',                                            # è„šæ³¨
    'sane_lists',                                           # æ™ºèƒ½åˆ—è¡¨æ ¼å¼åŒ–
    'nl2br',                                                # æ¢è¡Œè½¬<br>æ ‡ç­¾
    # æ”¯æŒç¼©å†™ï¼ˆå¦‚ [[NASA|National Aeronautics...]],<a title="National Aeronautics and Space Administration" href="http://www.nasa.gov/">NASA</a> ï¼‰
    'abbr',                                                 # æ”¯æŒç¼©å†™
    'toc',                                                  # ç›®å½•
    # 'strikethrough',                                        # åˆ é™¤çº¿
    'pymdownx.tasklist'                                     # ä»»åŠ¡åˆ—è¡¨(å¾…åŠåˆ—è¡¨ï¼ˆéœ€ pymdown-extensionsï¼‰)    
]
markdown_extconfigs = {
    'codehilite': {'linenums': True, 'pygments_style': 'monokai'}
}

# è¿œç¨‹ https://res.cdn.issem.cn/ChineseStopWords.txt, å¹¶å°†å†…å®¹è½¬æ¢ä¸ºåˆ—è¡¨
cn_stopk, en_stopk = [[*map(str.strip,filter(str.strip,(WorkDIR/config['stopk'][l]).open(encoding='utf-8')))] for l in ('cn','en')]
# æ—¥å¿—è®°å½•é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - L%(lineno)d - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logging.info(f"ğŸš€ å¯åŠ¨ç¨‹åº {__author__}")
# ##########################################################################################################################

class WeChatMP():
    def __init__(self, wechat: Dict, gpts: List[Dict]) -> None:
        self.gpts = gpts
        self.wechat = wechat

    """ä»¤ç‰Œï¼šè¿”å›çŠ¶æ€å’Œä»¤ç‰Œ"""
    def get_wechatmp_token(self) -> Tuple[bool, str]:
        try:
            with (Path(WorkDIR) / f"{self.wechat.get('appid')}.json").open("r", encoding="utf-8") as f:
                data = json.load(f)
            expires_in = data.get("expires_in", 0)
            # (è®¾ç½®æå‰200ç§’è¿‡æœŸï¼Œç”¨æ¥æŠµæ‰£ç¨‹åºè¿è¡Œä¸­è€—æ—¶)
            if expires_in > 0 and expires_in > int(time.time()) + 200:        
                return True, data.get("access_token")
        except (FileNotFoundError, json.JSONDecodeError):
            try:
                with httpx.Client() as client:
                    url = f"{self.wechat['baseurl']}/cgi-bin/token"
                    params = {
                        'grant_type': 'client_credential',
                        'appid': self.wechat.get('appid'),
                        'secret': self.wechat.get('secret')
                    } 
                    res = client.get(url, params=params)
                    res.raise_for_status()
                    data = res.json()
                    if "access_token" in data:
                        # ä¿®æ”¹è¿‡æœŸæ—¶é—´ï¼šå°†è¿”å›çš„è¿‡æœŸæ—¶é•¿ä¿®æ”¹ä¸ºå½“å‰æ—¶é—´æˆ³åŠ ä¸Šè¿”å›çš„è¿‡æœŸæ—¶é•¿
                        data["expires_in"] = int(time.time()) + data["expires_in"]
                        with (Path(WorkDIR) / f"{self.wechat.get('appid')}.json").open("w", encoding="utf-8") as f:
                            json.dump(data, f)
                        return True, data.get("access_token")
                    else:
                        return False, data.get("errmsg", "")
            except (httpx.RequestError, httpx.HTTPStatusError) as e:
                return False, "è·å–access_tokenå¤±è´¥"
            

    """è‰ç¨¿:è¿”å›çŠ¶æ€å’Œæ–‡ç« ID"""
    def get_wechatmp_draft(self, articles: List[Dict]) -> Tuple[bool, str]:
        token = self.get_wechatmp_token()[1]
        if not token[0]:
            return False, "è·å–access_tokenå¤±è´¥"
        _drafts = []
        # ä¸Šä¼ å›¾ç‰‡ è¿”å›(sucess, media_id)
        @staticmethod
        def upload_wechatmp_image(token: str, file_path: str) -> Tuple[bool, str]:
            try:
                with open(file=file_path, mode='rb') as fp:
                    url = f"{self.wechat['baseurl']}/cgi-bin/material/add_material"
                    params = {
                        'access_token': token,
                        'type': 'image'
                    }         
                    files = {'media': fp}
                    with httpx.Client() as client:
                        res = client.post(url=url, params=params, files=files)
                        res.raise_for_status()
                        try:
                            data = res.json()
                            if "media_id" in data:
                                 return True, data.get("media_id", "")
                            else:
                                 return False, data.get("errmsg", "")
                        except json.JSONDecodeError:
                            return False, "å“åº”å†…å®¹æ— æ³•è§£æä¸ºJSON"
            except (httpx.RequestError, httpx.HTTPStatusError, FileNotFoundError, Exception) as e:
                # logging.error()
                return False, f"å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {e}"

        for article in articles:
            try:
                # åŒ¹é…æœ€ç›¸ä¼¼çš„é”®
                media_ids = self.wechat.get("media_ids")
                _titmatch = process.extractOne(article["title"], media_ids.keys())
                image_stauts, cover_media_id = True, (media_ids[_titmatch[0]] if _titmatch else random.choice(media_ids.values()))
            except:
                image_stauts, cover_media_id = upload_wechatmp_image(token, article["cover_path"]) 

            if not image_stauts: return False

            _drafts.append({
                "article_type": "news",
                "title": article["title"],
                "thumb_media_id": cover_media_id,                   # å›¾æ–‡æ¶ˆæ¯çš„å°é¢å›¾ç‰‡ç´ æidï¼ˆå¿…é¡»æ˜¯æ°¸ä¹…MediaIDï¼‰
                "content": article["content"],                      #{company_info}{top_ad}{bottom_ad}
                "content_source_url": self.wechat.get("source_url"),    # å›¾æ–‡æ¶ˆæ¯çš„åŸæ–‡åœ°å€ï¼Œå³ç‚¹å‡»â€œé˜…è¯»åŸæ–‡â€åçš„URL
                "need_open_comment": 1,                             # æ˜¯å¦æ‰“å¼€è¯„è®ºï¼Œ0ä¸æ‰“å¼€ï¼Œ1æ‰“å¼€
                "only_fans_can_comment": 0,                         # æ˜¯å¦ç²‰ä¸æ‰å¯è¯„è®ºï¼Œ0æ‰€æœ‰äººå¯è¯„è®ºï¼Œ1ç²‰ä¸æ‰å¯è¯„è®º
                "author": self.wechat.get("author"),                    # éå¿…å¡«
                # "digest": "digest",                               # éå¿…å¡«(ä»…æœ‰å•å›¾æ–‡æ¶ˆæ¯æ‰æœ‰æ‘˜è¦ï¼Œåˆ™é»˜è®¤æŠ“å–æ­£æ–‡å‰54ä¸ªå­—ã€‚)
                # "pic_crop_235_1": "X1_Y1_X2_Y2",                  # å°é¢å›¾ç‰‡çš„è£å‰ªåæ ‡ï¼Œåæ ‡ä»¥å›¾ç‰‡å·¦ä¸Šè§’ä¸ºåŸç‚¹ï¼Œxè½´å‘å³ï¼Œyè½´å‘ä¸‹
                # "pic_crop_1_1": "X1_Y1_X2_Y2"                     # å°é¢å›¾ç‰‡çš„è£å‰ªåæ ‡ï¼Œåæ ‡ä»¥å›¾ç‰‡å·¦ä¸Šè§’ä¸ºåŸç‚¹ï¼Œxè½´å‘å³ï¼Œyè½´å‘ä¸‹
            })

        drafts = {"articles": _drafts}
        # ç›´æ¥post json.dumps(drafts) ä¼šæœ‰ä¹±ç     
        post_data = json.dumps(drafts, ensure_ascii=False).encode('utf-8')
        try:
            with httpx.Client() as client:
                url = f"{self.wechat['baseurl']}/cgi-bin/draft/add"
                params = {
                    'access_token': token
                }     
                res = client.post(url=url, params=params, content=post_data)
                res.raise_for_status()
                try:
                    data = res.json()
                    if "media_id" in data:
                        return True, data.get("media_id", "")
                    else:
                        return False, data.get("errmsg", "")
                except json.JSONDecodeError:
                    return False, "å“åº”å†…å®¹æ— æ³•è§£æä¸ºJSON"
        except Exception as e:
            return False, f"è‰ç¨¿ä¸Šä¼ å¤±è´¥: {e}"
        

    """å‘å¸ƒ:è¿”å›çŠ¶æ€å’Œæ–‡ç« ID"""
    def get_wechatmp_publish(self, draft_id: int) -> Tuple[bool, str]:
        token = self.get_wechatmp_token()[1]
        if not token[0]:
            return False, "è·å–access_tokenå¤±è´¥"
        try:
            with httpx.Client() as client:
                url = f"{self.wechat['baseurl']}/cgi-bin/freepublish/add"
                params = {
                    'access_token': token
                }
                res = client.post(url=url, params=params ,json={"media_id": draft_id})
                res.raise_for_status()
                try:
                    data = res.json()
                    if "media_id" in data and data.get("errcode") == 0:
                        return True, data.get("publish_id", "")
                    else:
                        return False, data.get("errmsg", "")
                except json.JSONDecodeError:
                    return False, "å“åº”å†…å®¹æ— æ³•è§£æä¸ºJSON"
        except Exception as e:
            return False, f"æ–‡ç« å‘å¸ƒå¤±è´¥ï¼š{e}" 


# ========================================================================================================================
# ä»¥ä¸‹ä¸ºæ‹“å±•ç±»ï¼ŒæŒ‰éœ€ä½¿ç”¨
# ========================================================================================================================

    """å‘å¸ƒåˆ°æ–‡ç« åˆ°å…¬ä¼—å·ä¸Š"""
    def main(self):
        # 1. è·å–æ–‡ç« æ•°æ®
        articles = Extensions.get_article_data(self.wechat)
        if not articles:
            return False
        if self.wechat['draft']:# 2 ä¿å­˜å¾®ä¿¡è‰ç¨¿
            draft_status, draft_id = self.get_wechatmp_draft(articles)
            if not draft_status: 
                return False
        if self.wechat['publish']:# 3 å¾®ä¿¡æ¨é€å‘å¸ƒ
            publish_status, publish_id = self.get_wechatmp_publish(draft_id)
            if not publish_status: 
                return "å‘å¸ƒå¤±è´¥", None
        return "å‘å¸ƒæˆåŠŸ", publish_id


class Extensions:
    """è·å–æ‰€æœ‰å¯ç”¨GPTé…ç½®"""
    @staticmethod
    def get_gpt_config(gpts: List[Dict]) -> List[Dict]:
        new_gpts = []
        for entry in gpts:
            if any('sk-or-' in key for key in entry['apikey']) and len(entry["models"])==0:
                get_models = Extensions.get_openrouter_models(
                    entry["baseurl"],
                    entry["fee_type"], 
                    entry["from_type"], 
                    entry["to_type"], 
                    entry["min_tokens"]
                )
                _entry = {
                    "baseurl": entry["baseurl"],
                    "apikey": entry["apikey"],
                    "models": get_models
                }
                new_gpts.append(_entry)
            else:
                new_gpts.append(entry)
        return new_gpts
    

    """è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    @staticmethod
    def get_openrouter_models(
        baseurl: str = "https://openrouter.ai/api/v1", 
        fee_type: str = "free", 
        from_type: str = "text", 
        to_type: str = "text", 
        min_tokens: int = 4096
    ) -> List[Dict]:
        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        type_models = []
        try:
            with httpx.Client() as client:
                resp = client.get(url=f"{baseurl}/models")
                resp.raise_for_status()
                models_data = resp.json()
        except Exception as e:
            return []

        _free, _paid = [], []
        # ç¬¬ä¸€æ­¥ï¼šç­›é€‰æ¨¡å‹ç±»å‹
        for _f in models_data.get("data", []):
            pricing = _f.get("pricing")
            if pricing.get("prompt") == "0" and pricing.get("completion") == "0":
                _free.append(_f)
            else:
                _paid.append(_f)   
        models_detail = {"paid": _paid, "free": _free}.get(fee_type, _free + _paid)
            
        # ç¬¬äºŒæ­¥ï¼šç­›é€‰æ¨¡å‹ç±»å‹
        for model in models_detail:
            modality = model.get("architecture").get("modality")
            max_completion_tokens = model.get("top_provider").get("max_completion_tokens")
            _tokens = max_completion_tokens is None or max_completion_tokens > min_tokens
            _type = from_type + "->" + to_type
            if modality == _type and _tokens:
                # type_models.append(model)
                type_models.append({
                    "baseurl": "https://openrouter.ai/api/v1",
                    "id": model.get("id"),
                    "name": model.get("name"),
                    "pricing": model.get("pricing"),
                    "max_tokens": max_completion_tokens,
                    "context_length": model.get("context_length")
                })
            else:
                continue
        
        # return list(type_models)
        return [models.get("id") for models in type_models]
    
    
    """ç”¨åˆ†è¯å™¨æå–å…ƒå…³é”®è¯"""
    @staticmethod
    def extract_keywords(content: str, require_words: List[str] ) -> List[str]:
        if not content: return []
        # å¤„ç†æˆMarkdownæ ¼å¼
        try:
            html = markdown.markdown(content)
            if not (('<' in html and '>' in html) and html.strip() != content.strip()):
                content = markdownify(content)
        except Exception:
            content = markdownify(content)
        # æ¸…ç†éæ³•å­—ç¬¦ï¼ˆä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€å¸¸è§ç¬¦å·ï¼‰
        # éœ€è¦åŒ¹é…è‹±æ–‡å•è¯ã€ä¸­æ–‡ã€ä¸“æœ‰è‹±æ–‡ç¼©å†™ï¼ˆå¦‚ NASAã€U.S.A.ï¼‰ä»¥åŠç‰¹å®šç¬¦å·
        content = re.sub(
            r'[^a-zA-Z0-9\u4e00-\u9fa5\s\-.\'@#$%&*+/:;=?~(){}$`_ã€ã€‚ï¼Œã€Šã€‹ï¼Ÿï¼â€œâ€â€˜â€™ï¼ˆï¼‰â€”â€¦]',
            ' ',                                                # ç”¨ç©ºæ ¼æ›¿ä»£éæ³•å­—ç¬¦
            content,                                            # æ›¿æ¢ç›®æ ‡å­—ç¬¦ä¸²
            flags=re.UNICODE
        ).strip()
        # åˆ¤æ–­æ–‡æœ¬è¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰
        cn_lang = any(
            (u'\u4e00' <= char <= u'\u9fa5') or
            (u'\u3400' <= char <= u'\u4DBF') or
            (u'\U00020000' <= char <= u'\U0002A6DF')
            for char in content
        )
        # å¯é€‰åœç”¨è¯
        if not cn_lang:
            stop_words = set(stopwords.words('english')).union(en_stopk)
        else:
            stop_words = set(stopwords.words('chinese')).union(cn_stopk)
        # ===============================================================================================================
        # Keybert ç®—æ³•ï¼ˆè‹±æ–‡ï¼‰ / TextRank + jieba ç®—æ³•ï¼ˆä¸­æ–‡ï¼‰
        # ===============================================================================================================
        if not cn_lang:
            # è‹±æ–‡å¤„ç†ï¼šKeyBERT + è¯­ä¹‰ä¼˜å…ˆ
            kw_model = KeyBERT()
            keywords = kw_model.extract_keywords(
                content.lower(),
                keyphrase_ngram_range=(1, 6),
                stop_words=stop_words,
                use_mmr=True,
                diversity=0.5
            )
            vectorizer = TfidfVectorizer(ngram_range=(1, 6))
            tfidf = vectorizer.fit_transform([content])
            vocab = vectorizer.vocabulary_  # è·å–è¯æ±‡è¡¨
            # ç»¼åˆè¯„åˆ†ï¼šBERTç½®ä¿¡åº¦ Ã— TF-IDF
            scores = {
                word: score * tfidf[0, vocab.get(word, -1)]
                for word, score in keywords
                if word in vocab and word.lower() in content.lower()  # ç¡®ä¿å…³é”®è¯åœ¨æ–‡ç« ä¸­
            }
        else:
            # ä¸­æ–‡å¤„ç†ï¼šTextRank + è¯å¯†åº¦
            # å°†ç”¨æˆ·è¯é€ä¸ªæ·»åŠ åˆ°jiebaè¯å…¸ï¼ˆç¡®ä¿çŸ­è¯­ä¸è¢«æ‹†åˆ†ï¼‰
            list(map(jieba.add_word, require_words))
            keywords = [word for word in jieba.cut(content) if word not in stop_words or word in require_words]
            vectorizer = TfidfVectorizer(ngram_range=(1, 6), token_pattern=r'[^\s]+')
            tfidf_matrix = vectorizer.fit_transform([' '.join(keywords)])
            vocab = vectorizer.vocabulary_  
            tfidf = tfidf_matrix.toarray()[0]
            # TextRankæƒé‡ï¼ˆå…è®¸çŸ­è¯­ï¼‰
            text_rank = {k: v for k, v in jieba.analyse.extract_tags(' '.join(keywords), topK=500, withWeight=True, allowPOS=())}
            scores = {
                word: text_rank.get(word, 0) * tfidf[vocab[word]] 
                for word in keywords 
                 if word in vocab and word in content  # ç¡®ä¿å…³é”®è¯åœ¨æ–‡ç« ä¸­
            }
        # å¼ºåˆ¶åˆå¹¶ç”¨æˆ·è¯å…¸ä¸­çš„è¯ï¼ˆä¸­è‹±æ–‡ç»Ÿä¸€å¤„ç†ï¼‰
        for key in require_words:
            word = key.lower()
            if word in content.lower() and word not in scores:  # ç¡®ä¿ç”¨æˆ·è¯åœ¨æ–‡ç« ä¸­
                scores[word] = tfidf[vocab[word]] if cn_lang else 1.0 * tfidf[0, vocab[word]]
        # å¼ºåˆ¶åŒ…å«ç”¨æˆ·æä¾›è¯ï¼ˆç»Ÿä¸€é€»è¾‘ï¼‰
        for req_word in (r.lower() for r in require_words):
            if req_word in content.lower():  # ç¡®ä¿ç”¨æˆ·è¯åœ¨æ–‡ç« ä¸­
                current = scores.get(req_word, 0)
                scores[req_word] = current * 2 if current else max(scores.values(), default=0) * 2 + 1

        # ç”Ÿæˆæ’åºåçš„å…³é”®è¯åˆ—è¡¨
        t_k = sorted(scores.keys(), key=lambda k: (-scores[k], -len(k)))
        # è¿‡æ»¤ä¸ç¬¦åˆæ¡ä»¶çš„å…³é”®è¯
        filter_pattern = re.compile(r'^[\W_]+$|^\d+(?:\.\d+)?%?$|^\d+[eE][+-]?\d+$')
        return [k for k in t_k if len(k) >= 2 and not filter_pattern.fullmatch(k)]
        # ===============================================================================================================


    @staticmethod
    def extract_excerpt(content: str, length: int = 3) -> str:
        # å…ˆå°†markdownæ ¼å¼è½¬æ¢ä¸ºçº¯æ–‡æœ¬æ ¼å¼
        sentences = sent_tokenize(markdownify(content))
        # åˆ¤æ–­æ–‡æœ¬è¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰
        cn_lang = any(
            (u'\u4e00' <= char <= u'\u9fa5') or
            (u'\u3400' <= char <= u'\u4DBF') or
            (u'\U00020000' <= char <= u'\U0002A6DF')
            for char in content
        )
        # åˆ†å¥å¤„ç†
        if not cn_lang:
            sentences = sent_tokenize(content)
            stop_words = set(stopwords.words('english')).union(en_stopk)
        else:
            sentences = [s.strip() for s in re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]\s*', content) if s.strip()]
            stop_words = set(stopwords.words('chinese')).union(cn_stopk)
        # åˆ†è¯å¹¶å»é™¤åœç”¨è¯
        _sents = []
        for sent in sentences:
            if not cn_lang:
                tokens = [word for word in word_tokenize(sent.lower()) if word not in stop_words]
            else:
                tokens = [word for word in jieba.cut(sent) if word not in stop_words]
            _sents.append(tokens)
        # è®¡ç®— BM25
        bm25 = BM25Okapi(_sents)
        # è®¡ç®—æ¯ä¸ªå¥å­çš„å¾—åˆ†
        scores = []
        for query in _sents:
            scores.append(bm25.get_scores(query).mean())  # ä½¿ç”¨å¹³å‡å¾—åˆ†
        # è·å–å¾—åˆ†æœ€é«˜çš„å¥å­ç´¢å¼•å¹¶è¿”å›æ‘˜è¦
        excerpt = ' '.join([sentences[i] for i in sorted(np.argsort(scores)[::-1][:length])])
        return excerpt


    """é€šè¿‡AIç”Ÿæˆæ ‡é¢˜æˆ–å†…å®¹"""
    @staticmethod
    def get_gpt_generation(keyword: str, lang: str = "",mode: str = "body") -> Optional[str]:
        gpt = random.choice(gpts)
        role = f"ä½ æ˜¯ä¸€ä¸ªç²¾é€šç›¸å…³é¢†åŸŸå†…çŸ¥è¯†å’ŒæŠ€èƒ½çš„{lang}èµ„æ·±æ–‡æ¡ˆç¼–è¾‘ã€‚"
        if mode == "body":
            prompts = f"""ä»¥<{keyword}>ä¸ºæ ‡é¢˜ï¼Œå†™ä¸€ç¯‡{lang}çˆ†æ¬¾ç§‘æ™®æ€§æ–‡ç« ï¼Œä»¥Markdownæ ¼å¼æºç è¿”å›,åªè¦è¾“å‡ºæ–‡ç« å†…å®¹ï¼Œä¸è¦è¾“å‡ºæ ‡é¢˜ã€‚
é‡‡ç”¨æ–‡æ¡ˆåˆ›ä½œé»„é‡‘ä¸‰ç§’åŸåˆ™ã€‚
è¦æ±‚ç«™åœ¨ç”¨æˆ·è§’åº¦æ€è€ƒå’Œè§£è¯»ï¼Œç›´å‡»ç”¨æˆ·ç—›ç‚¹ï¼Œå…±æƒ…ç”¨æˆ·æƒ…ç»ªï¼Œå¹¶ä¸”å¼•å¯¼ç”¨æˆ·å…³æ³¨å’Œå’¨è¯¢ã€‚
æ–‡ç« ä¸­éœ€è¦å®‰æ’æ”¾ç½®æ’å›¾çš„åœ°æ–¹è¯·ä»¥ â€œ[æ–‡ç« æ’å›¾]â€ æç¤ºï¼ˆå•ç‹¬ä¸€è¡Œï¼‰ï¼Œæˆ‘ä¼šåœ¨åç»­å¤„ç†ã€‚
æ–‡å†…äºŒç»´ç å›¾ç‰‡æˆ‘ä¼šä½¿ç”¨å…¶ä»–ç”Ÿæˆï¼Œè¯·ä¸è¦å†åœ¨æ–‡ç« ä¸­æç¤ºæ”¾ç½®ã€‚
æ’ç‰ˆéœ€è¦è€ƒè™‘é˜…è¯»ä½“éªŒï¼Œåˆç†å®‰æ’é‡ç‚¹ä¿¡æ¯è¯è¯­æˆ–è€…æ®µè½é«˜äº®ï¼Œæ¯è¡Œéƒ½éœ€è¦é—´éš”ä¸€ä¸ªç©ºè¡Œï¼ˆä»£ç é™¤å¤–ï¼‰ã€‚
ä¸è¦å‡ºç°ä¸å†…å®¹æ— å…³çš„è§£é‡Šæ€§è¯­å¥å’Œæ–‡ç« æç¤ºã€‚
æ­¤å¤–ï¼Œæ–°å†™çš„æ–‡ç« éœ€è¦å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1ã€ç»“æ„æ¸…æ™°æ˜äº†ï¼Œå†…å®¹ä¹‹é—´è¿‡æ¸¡è‡ªç„¶ï¼Œè¯»è€…å¯ä»¥è½»æ¾ç†è§£æ–‡ç« çš„æ€è·¯ã€‚
2ã€åŒ…å«å……è¶³çš„ä¿¡æ¯å’Œè¯æ®ï¼Œèƒ½å¤Ÿæ”¯æ’‘æ–‡å†…è§‚ç‚¹å’Œè®ºæ®ï¼ŒåŒæ—¶å…·æœ‰ç‹¬ç‰¹çš„è§è§£å’Œè§‚ç‚¹ã€‚
3ã€ä½¿ç”¨ç®€ç»ƒã€å‡†ç¡®ã€æ˜ç¡®çš„è¯­è¨€ï¼Œè¯­æ³•æ­£ç¡®ï¼Œæ‹¼å†™æ— è¯¯ï¼Œè®©è¯»è€…å¯ä»¥è½»æ¾ç†è§£æ–‡ç« æ„å›¾ã€‚
4ã€é£æ ¼é€‚å½“ï¼ŒåŒ…æ‹¬ç”¨è¯ã€è¯­æ°”ã€å¥å¼å’Œç»“æ„ï¼Œé€‚åº”è¯»è€…çš„èƒŒæ™¯å’Œé˜…è¯»ç›®çš„ã€‚
5ã€é¿å…è¿‡äºå­¦æœ¯åŒ–çš„è¡¨è¾¾ï¼Œé¿å…æœºæ¢°åŒ–çš„è¡¨è¾¾ï¼Œé¿å…ä½¿ç”¨è¿‡æ—¶çš„æŠ€æœ¯ã€æ–¹æ³•ã€æœ¯è¯­ç­‰ã€‚
6ã€å¼•å…¥BERTè¯­ä¹‰æ‹“æ‰‘å»ºæ¨¡ã€TF-IDFå‘é‡åˆ†æã€ä¸‰å…ƒç»„çŸ¥è¯†å›¾è°±å’ŒLDAä¸»é¢˜æ¨¡å‹ç­‰é«˜çº§æŠ€æœ¯è¿›ä¸€æ­¥æå‡SEOæ•ˆæœã€‚
7ã€æ–‡ç« å†™æ³•æŒ‡å—ï¼šæœç»æ ‡é¢˜å…šï¼ˆå†…å®¹éœ€ä¸æ ‡é¢˜åŒ¹é…ï¼‰ï¼Œé¿å…å…³é”®è¯å †ç Œï¼ˆé˜²æ­¢æœºæ¢°æ„Ÿï¼‰ï¼Œæ‹’ç»è‡ªå—¨ï¼ˆæµ‹è¯•ä¸‰äººæ³•åˆ™ï¼šä¸‰äººè¶…3ç§’çŠ¹è±«åˆ™é‡å†™ï¼‰ã€‚
8ã€è¯·ç´§æ‰£æ ‡é¢˜å’Œå…³é”®è¯ï¼Œåˆ‡å¿Œä¸è¦åé¢˜ã€è·‘é¢˜ã€‚
"""
        elif mode == "title":
            prompts = f"""ä»¥<{keyword}>ä¸ºå…³é”®è¯ï¼Œè¯·å‚è€ƒä»¥ä¸‹å†™æ³•å¹¶ç»“åˆè¡Œä¸šç‰¹ç‚¹å†™æ³•æ‹Ÿä¸€ä¸ª{lang}çˆ†æ¬¾æ ‡é¢˜ï¼Œè¯·ä¸è¦ä½¿ç”¨â€˜ï¼šâ€™åˆ†éš”æ ‡é¢˜ï¼Œè¦å°½é‡äººæ€§åŒ–å’Œèƒ½å‹¾èµ·è¯»è€…å…´è¶£ï¼Œä½ åªéœ€è¦æŠŠæ ‡é¢˜è¾“å‡ºæ¥ï¼Œæ— éœ€å…¶ä»–ä¿¡æ¯ã€‚
ä»¥ä¸‹æ˜¯ä¸€äº›æ ‡é¢˜å†™æ³•å‚è€ƒ(æŒ‰ç…§è¿™ä¸ªå¥å¼ï¼Œâ€œï¼šâ€å‰ä¸æ˜¯æ ‡é¢˜å†…å®¹ï¼Œè€Œæ˜¯è¦çªå‡ºçš„æ ¸å¿ƒæ€æƒ³)ï¼š
- ç¬¬ä¸€æ‹›ï¼šæ•°å­—æ³•è¿›é˜¶ç‰ˆ
ç”¨å…·ä½“å¤©æ•°åˆ¶é€ çœŸå®æ„Ÿï¼šæœˆè–ª5åƒåˆ°5ä¸‡ï¼Œæˆ‘ç”¨äº†237å¤©
æ•°å­—ç»„åˆæ„å»ºæ•…äº‹æ„Ÿï¼šé¢è¯•è¢«æ‹’8æ¬¡åï¼Œæˆ‘æ‚Ÿå‡ºäº†è¿™3ä¸ªæ½œè§„åˆ™
è¦ç‚¹ï¼šå¥‡æ•°å­—æ•° > å¶æ•°å­—æ•°ï¼›å…·ä½“æ•°å€¼ > ç¬¼ç»Ÿæ•´æ•°
- ç¬¬äºŒæ‹›ï¼šæ‚¬å¿µè®¾è®¡æ³•
å†²çªå‰ç½®+æƒ…ç»ªåç½®ï¼šé¢†å¯¼åŠå¤œç»™æˆ‘å‘äº†æ¡å¾®ä¿¡ï¼Œæˆ‘æ•´æ™šæ²¡ç¡ç€
è¿ç»­å‰§å¼æ‚¬å¿µï¼šå‡ºå·®å›æ¥å‘ç°æ¢³å¦†å°ä¸Šå¤šäº†æ”¯å£çº¢ï¼ŒæŸ¥å®Œç›‘æ§æˆ‘å“­äº†
è¦ç‚¹ï¼šå‰åŠå¥åŸ‹å†²çªï¼ŒååŠå¥ç»™æƒ…ç»ªå‡ºå£
- ç¬¬ä¸‰æ‹›ï¼šç—›ç‚¹åœºæ™¯åŒ–
å‡å­¦ç„¦è™‘ï¼šå­©å­è€ƒä¸ä¸Šé‡ç‚¹é«˜ä¸­ï¼Ÿè¿™5ä¸ªè‡ªæ•‘æ–¹æ¡ˆç°åœ¨çœ‹è¿˜æ¥å¾—åŠ
ä¸­å¹´å±æœºï¼š35å²è¢«è£å‘˜ï¼Œé è¿™ä¸‰ä¸ªé‡è·¯å­æˆ‘åè€Œå¤šèµšäº†20ä¸‡
è¦ç‚¹ï¼šç”¨ã€Œå‡Œæ™¨ä¸‰ç‚¹æ”¹PPTã€æ›¿ä»£ã€ŒèŒåœºå‹åŠ›ã€ï¼Œç”¨ã€Œè¾…å¯¼ä½œä¸šå¿ƒæ¢—ã€æ›¿ä»£ã€Œæ•™è‚²ç„¦è™‘ã€
- ç¬¬å››æ‹›ï¼šæƒ…ç»ªè¿‡å±±è½¦
æ†‹å±ˆâ†’è§£æ°”ï¼šåŒäº‹æŠŠæˆ‘åšçš„æ–¹æ¡ˆæ®ä¸ºå·±æœ‰ï¼Œæˆ‘çš„åå‡»è®©å…¨å…¬å¸é¼“æŒ
ç»æœ›â†’æƒŠå–œï¼šè¢«æˆ¿ä¸œèµ¶å‡ºæ¥çš„ç¬¬7å¤©ï¼Œæˆ‘ä½è¿›äº†æœˆç§Ÿ500çš„è±ªå®…
è¦ç‚¹ï¼šå•æ¬¡è§¦å‘ä¸€ä¸ªæƒ…ç»ªæŒ‰é’®ï¼ˆæ„¤æ€’/ææƒ§/å¥½å¥‡/ä¼˜è¶Šæ„Ÿï¼‰
- ç¬¬äº”æ‹›ï¼šçƒ­ç‚¹é™ç»´æœ¯
å†¬å¥¥æ¡ˆä¾‹ï¼šè°·çˆ±å‡Œæ¯å¤©ç¡10å°æ—¶ï¼Ÿæ‰“å·¥äººå­¦è¿™ä¸‰ç‚¹å°±å¤Ÿäº†
çƒ­æ’­å‰§å«æ¥ï¼šã€Šç‹‚é£™ã€‹é«˜å¯å¼ºæ··ç¤¾ä¼šçš„ä¸‰ä¸ªåº•å±‚é€»è¾‘ï¼Œç”¨åœ¨èŒåœºçœŸé¦™
è¦ç‚¹ï¼šæå–çƒ­ç‚¹æ ¸å¿ƒæƒ…ç»ªè€Œéç›´æ¥è¯„è®º
- ç¬¬å…­æ‹›ï¼šè®¤çŸ¥åå·®æœ¯
è–ªèµ„å¯¹æ¯”ï¼šæœˆè–ª3000å’Œæœˆè–ª3ä¸‡çš„äººï¼Œå·®çš„ä¸åªæ˜¯é’±
åå¸¸è¯†æš´å‡»ï¼šå¤©å¤©åŠ ç­çš„äººåè€Œæœ€å…ˆè¢«è£ï¼Ÿè€æ¿ä¸ä¼šè¯´çš„æ½œè§„åˆ™
åˆ¶é€ åé€»è¾‘å†²çªï¼šè¶Šçœé’±çš„äººè¶Šç©·ã€æ‹¼å‘½å·¥ä½œçš„äººå‡ä¸äº†èŒ
- ç¬¬ä¸ƒæ‹›ï¼šçµé­‚æ‹·é—®æ³•
æ‰å¿ƒæé—®ï¼šä¸ºä»€ä¹ˆä½ å­¦äº†100ä¸ªå†™ä½œè¯¾è¿˜æ˜¯å†™ä¸å¥½æ–‡æ¡ˆï¼Ÿ
çª¥æ¢å¼è®¾é—®ï¼šä½ çŸ¥é“è€æ¿æœ€æ€•å‘˜å·¥é—®å“ªä¸‰ä¸ªé—®é¢˜å—ï¼Ÿ
é¢„è®¾è¯»è€…å·²é‡åˆ°çš„é—®é¢˜ï¼šä¸ºä»€ä¹ˆè¶ŠåŠªåŠ›è¶Šç„¦è™‘ï¼Ÿ
- ç¬¬å…«æ‹›ï¼šå¼ºåŠ¿æŒ‡ä»¤æ³•
æ˜ç¡®è¡ŒåŠ¨ï¼šæ”¶è—ï¼è¿™20ä¸ªExcelå…¬å¼èƒ½çœä½ 80%å·¥ä½œé‡
å±æœºè­¦å‘Šï¼šç«‹åˆ»åœæ­¢ï¼è¿™äº”ç§æ—©é¤è¶Šåƒè¶Šèƒ–
è¦ç‚¹ï¼šæ­é…å¿…é¡»/åƒä¸‡/ç«‹åˆ»/é©¬ä¸Š+å…·ä½“åˆ©ç›Šç‚¹
- ç¬¬ä¹æ‹›ï¼šç¬¦å·èŠ‚å¥æœ¯
åŠ å·åˆ¶é€ æƒ³è±¡ï¼šæœˆå…¥3W+ï¼é€‚åˆæ‡’äººçš„æé’±å‰¯ä¸šï¼ˆé™„æ¸ é“ï¼‰
é—®å·å¼•å‘å¥½å¥‡ï¼š95åå¥³ç”Ÿç‹¬å±…vlogçˆ†ç«ï¼ŒåŸæ¥æ‹è§†é¢‘è¿™ä¹ˆç®€å•ï¼Ÿ
è¦ç‚¹ï¼šæ‹¬å·è¡¥å……ä¿¡æ¯é‡ï¼Œæ„Ÿå¹å·æ…ç”¨
- ç¬¬åæ‹›ï¼šæ¯›å­”çº§åœºæ™¯
å…·ä½“åœºæ™¯ï¼šè¹²é©¬æ¡¶åˆ·æ‰‹æœºæ—¶ï¼Œé¡ºæ‰‹å°±èƒ½åšçš„5ä¸ªæé’±å‰¯ä¸š
å¯¹è¯è¿˜åŸï¼šé¢†å¯¼è¯´'è¾›è‹¦äº†'åƒä¸‡åˆ«å›'åº”è¯¥çš„'
é”å®šå‘¨äº”ä¸‹ç­å‰ã€æ”¹ç¬¬äº”ç‰ˆæ–¹æ¡ˆã€å¾®ä¿¡ç¬¬3æ¡æ¶ˆæ¯ç­‰ç»†èŠ‚
""" 
        elif mode == "excerpt":
            prompts = f"""è¯·æ ¹æ®æˆ‘æä¾›çš„æ–‡ç« å†…å®¹å¹¶ç»“åˆæœç´¢å¼•æ“è§„åˆ™å¯¹æ–‡ç« åšä¸€ä¸ª100å­—å·¦å³{lang}ç®€å•æ¦‚è¿°ï¼Œå¹¶ä»¥çº¯æ–‡æœ¬è¿”å›ç»™æˆ‘ï¼Œåªè¦è¾“å‡ºæ¦‚è¿°å†…å®¹ï¼Œæ— éœ€åŸæ–‡å’Œå…¶ä»–ï¼Œä»¥ä¸‹æ˜¯æ–‡ç« å†…å®¹ï¼š\n{keyword}"""
        elif mode == "tags":
            prompts = f"""è¯·æ ¹æ®æˆ‘æä¾›çš„æ–‡ç« å†…å®¹å¹¶ç»“åˆæœç´¢å¼•æ“è§„åˆ™æå–å‡º5ä¸ªåˆé€‚çš„å…³é”®è¯ï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¹¶ä»¥çº¯æ–‡æœ¬è¿”å›ç»™æˆ‘ï¼Œåªè¦è¾“å‡ºå…³é”®è¯å†…å®¹ï¼Œæ— éœ€åŸæ–‡å’Œå…¶ä»–ï¼Œä»¥ä¸‹æ˜¯æ–‡ç« å†…å®¹ï¼š\n{keyword}"""
        else:
            prompts = ''
        client = OpenAI(
            api_key=random.choice(gpt["apikey"]), 
            base_url=gpt["baseurl"],
            http_client=httpx.Client(verify=False)
        )
        try:
            completion = client.chat.completions.create(
                model=random.choice(gpt["models"]),
                messages=[
                    {"role": "system","content": role},
                    {"role": "user","content": prompts}
                ],
                n=1,                    # è¾“å‡ºçš„æ¡æ•°
                temperature=0.5,        # è¾“å‡ºçš„éšæœºæ€§ï¼Œå€¼è¶Šä½è¾“å‡ºè¶Šç¡®å®š
                top_p=0.8,              # è¾“å‡ºçš„å¤šæ ·æ€§ï¼Œå€¼è¶Šä½è¾“å‡ºè¶Šé›†ä¸­
                max_tokens=4096,        # æ§åˆ¶ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡
                frequency_penalty=0.5,  # å‡å°‘é‡å¤å†…å®¹çš„ç”Ÿæˆ
                presence_penalty=0.5    # é¼“åŠ±æ¨¡å‹å¼•å…¥æ–°å†…å®¹
            )
            content = completion.choices[0].message.content
            # try:
            #     reasoning_content = completion.choices[0].message.reasoning_content
            # except:
            #     reasoning_content = ''
            # å»æ‰æ€ç»´é“¾
            return re.sub(r'<think>.*</think>', '', content, flags=re.DOTALL)
        except Exception as e:
            logging.error(f"{str(e)}")
            return None
        

    """æ•´åˆæˆå‘å¸ƒçš„æ•°æ®ç»“æ„"""
    @staticmethod
    def get_article_data(platform) -> List[Dict]:
        # æ–‡ç« å†…å®¹äºŒæ¬¡ç²¾ç»†åŒ–å¤„ç†
        @staticmethod
        def handle_article_content(content: str) -> str:
            # åŒ¹é…æ‰€æœ‰<h3>ã€<p>æ ‡ç­¾ï¼Œè®¾ç½®æ ·å¼
            content = re.sub(r'<p>', '<p style="box-sizing: border-box; border-width: 0px; border-style: solid; border-color: hsl(var(--border)); margin: 1.5em 8px; text-align: left; line-height: 1.75; font-family: -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; font-size: 14px; letter-spacing: 0.1em; color: rgb(63, 63, 63); visibility: visible;">', content)
            content = re.sub(r'<h3>', '<hr style="box-sizing: border-box; border-width: 2px 0px 0px; border-style: solid; border-color: rgba(0, 0, 0, 0.1); height: 0.4em; color: inherit; margin: 1.5em 0px; text-align: left; line-height: 1.75; font-family: -apple-system-font, BlinkMacSystemFont, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif; font-size: 14px; transform-origin: 0px 0px; transform: scale(1, 0.5); visibility: visible;">\n<h3>', content)
            # # æ‰¾åˆ°æ‰€æœ‰<p>æ ‡ç­¾çš„ä½ç½®ï¼Œå¹¶åœ¨ä¸­é—´çš„</p>åæ’å…¥å†…å®¹
            # p_tags = [m.end() for m in re.finditer(r'</p>', content)]
            # if p_tags:
            #     mid_insert_pos = p_tags[len(p_tags) // 2]
            #     content = content[:mid_insert_pos] + bottom_ad + content[mid_insert_pos:]
            # å¯¹<h3>æ ‡ç­¾è¿›è¡Œå¤„ç†ï¼šåŠ ç²—ã€ä¸‹åˆ’çº¿å’Œå±…ä¸­
            content = re.compile(r'(</p>\s*)(<p>)', re.DOTALL).sub(
                lambda m: f'{m.group(1)}<p><br /></p>{m.group(2)}',
                content
            )
            content = re.sub(r'<h3>(.*?)</h3>', r'<h3 style="text-align: center;font-size: 1.17em; font-weight: bold; margin-block: 1em;"><strong><span style="text-align:center;display:inline-blocktext-decoration:underline;">\1</span></strong></h3>', content)

        articles = []
        # è·å–æ‰€æœ‰å…³é”®è¯æˆ–è€…æ ‡é¢˜åˆ—è¡¨ï¼Œæ ¹æ®æ–‡ä»¶åï¼škeywords.txt / titles.txt
        # å½“å‰æ˜¯å…³é”®è¯æ¨¡å¼ï¼Œè¿˜æ˜¯æ ‡é¢˜æ¨¡å¼
        mode = 'keywords' if 'keywords' in platform.get('keys') else 'titles'
        if '.txt' not in platform.get('keys'):
            keys = list(set(platform.get('keys')))
        elif (key_path := WorkDIR / platform.get('keys')).exists():
            with key_path.open('r', encoding='utf-8') as _key:
                keys = list(set([s for l in _key if (s := re.sub(r'[\n\ufeff]', '', l)) and len(s) > 1]))
        else:
            return []
        # while True:
        for _ in range(platform.get('number', 1)): 
            key = random.choice(keys)                       # éšæœºé€‰æ‹©ä¸€ä¸ªåŸºç¡€å…³é”®è¯          
            _title = ''
            if mode == "titles":                            # çº¯æ ‡é¢˜ï¼Œä¸éœ€è¦ç”Ÿæˆæ ‡é¢˜
                _title = key
            else:                                           # çº¯å…³é”®è¯ï¼Œéœ€è¦ç”Ÿæˆæ ‡é¢˜
                for _t in range(3):
                    _tit = Extensions.get_gpt_generation(keyword=key, lang=platform.get('lang'), mode="title")
                    if _tit and not (_tit.startswith('<p>') or _tit.startswith('<h3>') or ' error' in _tit):
                        _title = re.sub(r'[\n<>"\'ã€Šã€‹{}ã€ã€‘ã€Œã€â€”â€”ã€‚]|&nbsp;|&lt;|&gt;|boxed', '', _tit)
                        if not (5<len(re.findall(r'[\u4e00-\u9fff]', _title))<30 or 5<len(re.findall(r"[A-Za-z'-]+", _title))<30):
                            logging.error(f"æ–‡ç« æ ‡é¢˜ ã€å­—æ•°ã€‘ ä¸ç¬¦åˆè¦æ±‚")
                            continue
                        break
                    else:
                        logging.error(f"æ–‡ç« æ ‡é¢˜ ã€å†…å®¹ã€‘ ä¸ç¬¦åˆè¦æ±‚")
                        continue
            logging.info(f"{_title}, {len(_title)}")
            # æ–‡ç« å†…å®¹è§„åˆ™
            for _t in range(3):
                _content = Extensions.get_gpt_generation(keyword=_title, lang=platform.get('lang'), mode="body")
                if _content and len(_content) > 100:
                    if not (len(re.findall(r'[\u4e00-\u9fff]', _content))>300 or len(re.findall(r"[A-Za-z'-]+", _content))>300):
                        logging.error(f"æ–‡ç« å†…å®¹ ã€å­—æ•°ã€‘ ä¸ç¬¦åˆè¦æ±‚")
                        continue
                    break
                else:
                    logging.error(f"æ–‡ç« è¯¦æƒ… ã€å†…å®¹ã€‘ ä¸ç¬¦åˆè¦æ±‚")
                    continue
            
            # æå–æ–‡ç« SEOå…³é”®è¯
            # åˆ†è¯åº“ï¼šå¿…è¦è¯
            if not platform.get('aitags'):
                if '.txt' not in platform.get('reqkeys'):
                    reqkeys = list(set(platform.get('reqkeys')))
                elif (reqkeys_path := WorkDIR / platform.get('reqkeys')).exists():
                    with reqkeys_path.open('r', encoding='utf-8') as req_key:
                        reqkeys = list(set([s for l in req_key if (s := re.sub(r'[\n\ufeff]', '', l)) and len(s) > 1]))
                else:
                    reqkeys = keys if mode == 'keywords' else []
                _tags = Extensions.extract_keywords(_content, reqkeys)[:5]
            else:
                _ts = Extensions.get_gpt_generation(keyword=_content, lang=platform.get('lang'), mode="tags")
                _tags = [x.strip() for x in re.split(r'[,\u3001]+', _ts) if x.strip()]
            if not platform.get('aiexcerpt'):
                _excerpt = Extensions.extract_excerpt(content=_content, length=5)
            else:
                _excerpt = Extensions.get_gpt_generation(keyword=_content, lang=platform.get('lang'), mode="excerpt")

            # å»æ‰æœºæ¢°å¼å¼€å¤´
            desc_preg = r'^.*?ã€?(?:æœ¬æ–‡|æ–‡ç« |æœ¬ç¯‡|å…¨æ–‡|å‰è¨€)\s*[ï¼Œ,]?\s*(?:ç®€ä»‹|æ‘˜è¦|æ¦‚è¿°|å¯¼è¯»|æè¿°)ã€‘?[ï¼š:]?'
            # æ£€æµ‹Markdownçš„å¸¸è§è¯­æ³•ï¼Œå¦‚æœä¸æ˜¯åˆ™è¿›è¡Œè½¬æ¢
            if re.search(r'#{1,6} |^-.*$|^```|^\|', _content, re.MULTILINE):
                _html = markdown.markdown(_content)
                if '<' in _html and '>' in _html and _html != _content:
                    _content = markdown.markdown(
                        re.sub(desc_preg,'', _content, flags=re.DOTALL | re.MULTILINE), 
                        output_format='html',
                        extensions=markdown_extensions,
                        extension_configs=markdown_extconfigs,
                        safe_mode='escape',
                        enable_attributes=True,
                        linkify=True,
                        tab_length=4,
                        lazy_ol=False,
                        toc=True,
                        toc_depth=4,
                        toc_title=u'æ–‡ç« ç›®å½•',
                        toc_nested=True,
                        toc_list_type='ul',
                        toc_list_item_class='toc-list-item',
                        toc_list_class='toc-list',
                        toc_header_id='toc',
                        toc_anchor_title=u'è·³è½¬è‡³æ–‡ç« ç›®å½•',
                        toc_anchor_title_class='toc-anchor-title'
                    )

            # æ–‡ç« å†å¤„ç†ï¼ˆç¬¦åˆå¹³å°éœ€è¦ï¼‰æœ‰BUG
            # _content  = f"""
            #     {platform.get('service_ad')}
            #     {handle_article_content(_content)}
            #     {platform.get('bottom_ad_top')}
            #     {platform.get('contact')}
            #     {platform.get('bottom_ad_bottom')}
            #     {platform.get("followme")}
            # """

            article = {
                "title": _title,                        # æ–‡ç« æ ‡é¢˜
                "keyword": key,                         # åŸå§‹å…³é”®è¯
                "content": _content,
                "tags": _tags if _tags else [key],
                "excerpt": _excerpt
            }
            # åˆ¤æ–­æ–‡ç« æ˜¯å¦ä¸ºç©º
            if not any(v == "" or v is None or (hasattr(v, '__len__') and len(v) == 0) for v in article.values()):
                articles.append(article)
                # å¤‡ä»½æ–‡ç« åˆ°æœ¬åœ°
                file_path = Path(WorkDIR) / "article" / f"{uuid.uuid4()}.md"  
                file_path.parent.mkdir(parents=True, exist_ok=True) 
                with file_path.open("w", encoding="utf-8") as f:
                    f.write(article.get('content'))
                # else:
                #     logging.error(f"å­—å…¸ä¸­å­˜åœ¨ç©ºå€¼ï¼Œè·³è¿‡è¯¥æ–‡ç« ")
                #     continue
        return articles
    

if __name__ == "__main__":
    # è·å–æ‰€æœ‰å¤§æ¨¡å‹é…ç½®
    gpts = Extensions.get_gpt_config(gpts=config.get('gpts'))
    # logging.info(f"è·å–æ‰€æœ‰å¯ç”¨GPTé…ç½®ï¼š{gpts[0]["models"]}")
    if gpts: 
        for wechat in config.get('mps'):
            WeChatMP(wechat, gpts).main()
